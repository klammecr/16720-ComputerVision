{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"img/course.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16720 (B)  Object Tracking in Videos - Assignment 6 - Q5\n",
    "    Instructor: Kris                          TAs: Arka, Rohan, Rawal, Sheng-Yu, Jinkun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Multi-Object Tracking by Detection (EC, 45 PT)\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this extra credit problem, you will be introduced to a more modern perspective on tracking. In the previous problems, you implemented single-object tracking with a classical method, the LK tracker. Multi-object tracking (MOT), on the other hand, is a richer and more useful problem to attack. One approach to this problem is called tracking by detection. In this paradigm, for each frame of a video, we produce object detections (typically from a learned object detection neural net). These are called proposals, and are often bounding boxes. In the next step, we associate those boxes with any existing tracked objects. For a more in-depth overview, please see the following [link](https://arshren.medium.com/an-introduction-to-object-tracking-9fd6249a76b6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.1 Loading the bounding boxes, video and visualization (5 pts)\n",
    "\n",
    "In the spirit of the World Cup, we will be evaluating your method on a short excerpt from a [famous soccer clip](https://youtu.be/uBa8dYlqv8Y). We'll begin by loading and visualizing the input. We have already computed bounding boxes for you, which are available in ```soccer_boxes.json```. The images to use in the video are available in ```soccer_images.npy``` Both files can be download at this [link](https://www.dropbox.com/sh/uovrr3cgtehhtuc/AAATS-GtGEwfS-z2MXRNku7Ea?dl=0).\n",
    "\n",
    "Fill in the functions below. For testing, your result for the visualization on the 124th frame should look something like\n",
    "\n",
    "<img align=\"center\" src=\"img/sample_bbox_img.jpg\" width=\"800\">\n",
    "\n",
    "Please submit an image of the bounding boxes rendered on the 1st frame of the sequence along with your code for this question to the writeup PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def load_images_and_boxes(img_path, box_path):\n",
    "    \"\"\"\n",
    "    Given a paths to the images and bounding boxes, loads them into numpy arrays\n",
    "    and returns for later use.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    boxes = None\n",
    "    with open(box_path, \"r\") as f:\n",
    "        boxes = json.load(f)        \n",
    "    imgs = np.load(img_path)\n",
    "    return imgs, boxes\n",
    "\n",
    "def render_single_frame(image, bboxes, colors=None):\n",
    "    \"\"\"\n",
    "    Given an image and bounding boxes, renders the bounding boxes on top of the image \n",
    "    and saves the image. \n",
    "    Also takes in an optional array of colors to apply to the boxes\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    for idx, bbox in enumerate(bboxes):\n",
    "        if colors is not None:\n",
    "            # Cycle through the colors if need be\n",
    "            color = colors[idx]\n",
    "        else:\n",
    "            color = None\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        image = cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (int(color[0]), int(color[1]), int(color[2])))\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.2 Implementing a similarity metric (10 pts)\n",
    "\n",
    "In order to do track association, we need a way to measure how similar two bounding boxes are to each other. One way to do this is intersection-over-union (IoU). An overview of how to compute IoU is provided [here](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/). Below, you will implement a function to compute IoU. The input will be a set of N bounding boxes (representing the boxes in the reference frame), and a set of M bounding boxes (representing the boxes in the next frame) and the output will be an NxM matrix, with the ```[i, j]```th entry correspoding to bounding box ```i``` in the reference frame's IoU with bounding box ```j``` of the next frame.\n",
    "\n",
    "For this question, please submit the matrix of IoUs between the boxes on the 124th and 125th frames, as well as your code to the writeup PDF, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        boxes1: Nx4 ndarray, representing N bounding boxes coordinates\n",
    "        boxes2: Nx4 ndarray, representing N bounding boxes coordinates\n",
    "    Output: \n",
    "        iou_mat: NxM ndarray, with iou_mat[i, j] = iou(boxes1[i], boxes2[j])\n",
    "    \"\"\"\n",
    "    # Placeholder for output ious\n",
    "    out_iou = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n",
    "\n",
    "    # Abandon, necessary for industry not for a homework\n",
    "    # # Calculate the bbox pairwise values for the intersection points\n",
    "    # xA = np.max(boxes1[:, 0], boxes2[:, 0])\n",
    "    # yA = np.max(boxes1[:, 1], boxes2[:, 1])\n",
    "    # xB = np.min(boxes1[:, 2], boxes2[:, 2])\n",
    "    # yB = np.max(boxes1[:, 3], boxes2[:, 3])\n",
    "\n",
    "    # inter_area = np.max(0, xB - xA + 1) * np.max(0, yB - yA + 1)\n",
    "    # box1_area  = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    # box2_area  = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    # union      = box2_area + box1_area - inter_area\n",
    "    # iou_vec    = inter_area / union\n",
    "    \n",
    "\n",
    "    # Function for the for loop\n",
    "    def iou_box_to_box(box1, box2):\n",
    "\n",
    "        # Get the coordinates of each box\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2\n",
    "\n",
    "        # Get the area of the intersection\n",
    "        xA = max(b1_x1, b2_x1)\n",
    "        xB = min(b1_x2, b2_x2)\n",
    "        yA = max(b1_y1, b2_y1)\n",
    "        yB = min(b1_y2, b2_y2)\n",
    "        inter_area = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "        # Calculate the union\n",
    "        b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "        b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "        union   = b1_area + b2_area - inter_area\n",
    "\n",
    "        return inter_area / union\n",
    "\n",
    "    # For loop implementation\n",
    "    for i in range(boxes1.shape[0]):\n",
    "        for j in range(boxes2.shape[0]):\n",
    "            out_iou[i, j] = iou_box_to_box(boxes1[i], boxes2[j])\n",
    "\n",
    "\n",
    "    return out_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.3 Matching with the Hungarian Algorithm (10 pts)\n",
    "\n",
    "Given a matrix of similarities, the next step in tracking by detection is to find the bounding box that corresponds most to the previous frame. In essence, the idea is to find the bounding box that has the closest IoU to a bounding box in a previous frame. The challenging part is to remove this bounding box from contention for other matches. This problem is known as the optimal cost assignment problem. The Hungarian algorithm is the most commonly used method to solve this problem, and is implemented in ```scipy.optimize.linear_sum_assignment```. Below, you will implement a function ```compute_assignment``` that will produce such a matching. \n",
    "\n",
    "Some notes to keep in mind: ```scipy```'s implementation uses costs, so you will need to use the negative of the similarities you computed in the previous part. Additionally, since the IoU of a bounding box with itself is 1, you will need to set the diagonal entries of the cost matrix to some high value so they are not picked. Finally, it's likely there will be matches with very low IoU scores. Please use the ```threshold``` parameter to filter out any matches that are below ```threshold``` IoU.\n",
    "\n",
    "Please submit your code for this section to the writeup PDF, along with the output run on the IOU matrix computed between the 124th and 125th images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def compute_assignment(iou_matrix, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Given an input matrix of IoUs, uses the Hungarian algorithm to compute a matching.\n",
    "    \n",
    "    Args:\n",
    "        iou_matrix: NxM matrix of IoUs of bounding boxes between two frames.\n",
    "        threshold: float value representing minimum value of IoU to be considered as a candidate for matching.\n",
    "\n",
    "    Returns:\n",
    "        box1_ind, box2_ind : a set of indices into box1 (bboxes of ref frame) and corresponding \n",
    "        indices into box2 representing the optimal assignment.\n",
    "    \"\"\"\n",
    "    iou_matrix[iou_matrix < threshold] = 0.0\n",
    "    cost_mtx = np.max(iou_matrix) - iou_matrix\n",
    "\n",
    "    box1_ind, box2_ind = linear_sum_assignment(cost_mtx)\n",
    "\n",
    "    return box1_ind, box2_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.4 Putting it all together (20 pts)\n",
    "\n",
    "Now, you will put all the pieces together that you implemented above to create a full tracking system. You will maintain a set of tracks throughout the video. At the beginning, no tracks will exist, only detections. In each successive frame, you will read in the detections for that frame and associate the new detections to the previous ones, and create candidate tracks. If a candidate track has persisted for P frames, you will add it to the list of tracks. If a track has not had a match in K frames, you will remove it from the list of tracks. For each frame, you will render the bounding boxes corresponding to every track. Once a track is removed from the list, do not render its bounding box. \n",
    "\n",
    "The hyperparameters you will use for the tracker are:\n",
    "\n",
    "P: number of frames a candidate track must exist before it is added to the list of tracks\n",
    "K: number of frames a track must have no match for before it is removed from the list of tracks\n",
    "iou_thresh: threshold to be used for whether a match is strong enough to be added to a track.\n",
    "\n",
    "\n",
    "For your submission for this part, please submit 10 images (with bounding boxes) from successive frames at any point in the video. Bounding boxes belonging to the same track should be the same color. Please also submit your code to the writeup PDF. \n",
    "\n",
    "Please note that the output will NOT be perfect! There should be ID switches and the tracker, even if implemented correctly, will likely fail in several frames. This should hopefully demonstrate to you that tracking is a tough problem and show why it is a hot research area today :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "class Trk:\n",
    "    def __init__(self, id, ind1, ind2):\n",
    "        # History of associations in the form of idxs\n",
    "        self.history = [ind1, ind2]\n",
    "        self.id = id\n",
    "        # Number of times missed\n",
    "        self.missed_frames = 0\n",
    "        self.color = np.random.randint(0, 255, size = 3)\n",
    "\n",
    "    def get_missed_frames(self):\n",
    "        return self.missed_frames\n",
    "\n",
    "    def get_persistence(self):\n",
    "        return len(self.history)\n",
    "\n",
    "    def get_current_box_id(self):\n",
    "        return self.history[-2]\n",
    "\n",
    "    def get_color(self):\n",
    "        return self.color\n",
    "\n",
    "    def update(self, update_dict):\n",
    "        ret = None\n",
    "        last_idx = self.history[-1]\n",
    "        if last_idx not in update_dict.keys():\n",
    "            self.missed_frames += 1\n",
    "        else: \n",
    "            next_idx = update_dict.get(last_idx)\n",
    "            self.history.append(next_idx)\n",
    "            self.missed_frames = 0\n",
    "            # Let the caller know that we did associate this detection\n",
    "            ret = last_idx\n",
    "        return ret\n",
    "\n",
    "def run_tracker(images, boxes, P, K, iou_thresh):\n",
    "    \"\"\"\n",
    "    Runs the entire tracking pipeline. \n",
    "\n",
    "    Args:\n",
    "        images: numpy array of N images. \n",
    "        boxes: list of bounding boxes for \n",
    "    \"\"\"\n",
    "    trks       = []\n",
    "    candidates = []\n",
    "    # Global variable for trk id\n",
    "    trk_id = 0\n",
    "    \n",
    "    for idx, img in enumerate(images):\n",
    "        # List of bounding boxes for the current frame\n",
    "        boxes_frame = np.array(boxes[idx][\"boxes\"])\n",
    "\n",
    "        # Compute the IoU Matrix\n",
    "        iou_mat = compute_iou(np.array(boxes[idx][\"boxes\"]), np.array(boxes[idx+1][\"boxes\"]))\n",
    "\n",
    "        # Assign bounding boxes to each other for tracking\n",
    "        box1_ind, box2_ind = compute_assignment(iou_mat, iou_thresh)\n",
    "        idx_mapping = dict(zip(box1_ind, box2_ind))\n",
    "\n",
    "        # Go through existing candidate tracks and update them\n",
    "        associated = np.zeros((boxes_frame.shape[0]))\n",
    "        promotion  = np.zeros((len(candidates)))\n",
    "        deletion_cand = np.zeros((len(candidates)))\n",
    "        for idx, candidate in enumerate(candidates):\n",
    "            # Update the candidate\n",
    "            assoc_idx = candidate.update(idx_mapping)\n",
    "            # Make a mark that we associated this detection\n",
    "            if assoc_idx is not None:\n",
    "                associated[assoc_idx] = 1\n",
    "\n",
    "            if candidate.get_persistence() >= P:\n",
    "                # Promote to full track and delete for candidates list\n",
    "                promotion[idx] = 1\n",
    "                deletion_cand[idx] = 1\n",
    "            if candidate.get_missed_frames() >= K:\n",
    "                deletion_cand[idx] = 1\n",
    "\n",
    "       # Do promotion\n",
    "        promotion_idxs = np.where(promotion==1)\n",
    "        for idx in promotion_idxs[0]:\n",
    "            trks.append(candidates[idx])\n",
    "\n",
    "        # Do deletion for candidates\n",
    "        candidate_delete_idxs = np.where(deletion_cand==1)\n",
    "        if len(candidate_delete_idxs[0]):\n",
    "            candidates = list(np.delete(candidates, candidate_delete_idxs))\n",
    "\n",
    "        # Visit the full tracks\n",
    "        deletion_trk = np.zeros((len(trks)))\n",
    "        for idx, trk in enumerate(trks):\n",
    "            assoc_idx = trk.update(idx_mapping)\n",
    "            # Make a mark that we associated this detection\n",
    "            associated[assoc_idx] = 1\n",
    "\n",
    "            # Delete tracks if they are stale\n",
    "            if trk.get_missed_frames() >= K:\n",
    "                deletion_trk[idx] = 1\n",
    "\n",
    "        # Do deletion for tracks\n",
    "        trk_delete_idxs = np.where(deletion_trk == 1)\n",
    "        if len(trk_delete_idxs[0]):\n",
    "            trks = list(np.delete(trks, trk_delete_idxs))\n",
    "        \n",
    "\n",
    "        # For all that were not associated, create new candidates\n",
    "        new_candidate_idxs = np.where(associated == 0)\n",
    "        for idx in new_candidate_idxs[0]:\n",
    "            if idx in idx_mapping.keys():\n",
    "                candidates.append(Trk(trk_id, idx, idx_mapping[idx]))\n",
    "            \n",
    "        # Display the tracks\n",
    "        trk_boxes = []\n",
    "        colors    = []\n",
    "        for trk in trks:\n",
    "            if not trk.get_missed_frames() > 0:\n",
    "                bbox_id = trk.get_current_box_id()\n",
    "                trk_boxes.append(boxes_frame[bbox_id])\n",
    "                color = trk.get_color()\n",
    "                colors.append((int(color[0]), int(color[1]), int(color[2])))\n",
    "        render_img = render_single_frame(img, trk_boxes, colors)\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.imshow(\"Rendered Image\", render_img)\n",
    "        cv2.waitKey(60)\n",
    "    \n",
    "\n",
    "# Part 1: Load the images and the boxes\n",
    "images, boxes = load_images_and_boxes(\"data/soccer_images.npy\", \"data/soccer_boxes.json\")\n",
    "run_tracker(images, boxes, P = 5, K = 5, iou_thresh=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a14d3cb3a8a01d6659ca21286c75f37ed68cd344cf818a36ad292d43e0de27a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
