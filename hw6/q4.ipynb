{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"img/course.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16720 (B)  Object Tracking in Videos - Assignment 6 - Q5\n",
    "    Instructor: Kris                          TAs: Arka, Rohan, Rawal, Sheng-Yu, Jinkun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Multi-Object Tracking by Detection (EC, 45 PT)\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this extra credit problem, you will be introduced to a more modern perspective on tracking. In the previous problems, you implemented single-object tracking with a classical method, the LK tracker. Multi-object tracking (MOT), on the other hand, is a richer and more useful problem to attack. One approach to this problem is called tracking by detection. In this paradigm, for each frame of a video, we produce object detections (typically from a learned object detection neural net). These are called proposals, and are often bounding boxes. In the next step, we associate those boxes with any existing tracked objects. For a more in-depth overview, please see the following [link](https://arshren.medium.com/an-introduction-to-object-tracking-9fd6249a76b6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.1 Loading the bounding boxes, video and visualization (5 pts)\n",
    "\n",
    "In the spirit of the World Cup, we will be evaluating your method on a short excerpt from a [famous soccer clip](https://youtu.be/uBa8dYlqv8Y). We'll begin by loading and visualizing the input. We have already computed bounding boxes for you, which are available in ```soccer_boxes.json```. The images to use in the video are available in ```soccer_images.npy``` Both files can be download at this [link](https://www.dropbox.com/sh/uovrr3cgtehhtuc/AAATS-GtGEwfS-z2MXRNku7Ea?dl=0).\n",
    "\n",
    "Fill in the functions below. For testing, your result for the visualization on the 124th frame should look something like\n",
    "\n",
    "<img align=\"center\" src=\"img/sample_bbox_img.jpg\" width=\"800\">\n",
    "\n",
    "Please submit an image of the bounding boxes rendered on the 1st frame of the sequence along with your code for this question to the writeup PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_boxes(img_path, box_path):\n",
    "    \"\"\"\n",
    "    Given a paths to the images and bounding boxes, loads them into numpy arrays\n",
    "    and returns for later use.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def render_single_frame(image, bboxes, colors=None):\n",
    "    \"\"\"\n",
    "    Given an image and bounding boxes, renders the bounding boxes on top of the image \n",
    "    and saves the image. \n",
    "    Also takes in an optional array of colors to apply to the boxes\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.2 Implementing a similarity metric (10 pts)\n",
    "\n",
    "In order to do track association, we need a way to measure how similar two bounding boxes are to each other. One way to do this is intersection-over-union (IoU). An overview of how to compute IoU is provided [here](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/). Below, you will implement a function to compute IoU. The input will be a set of N bounding boxes (representing the boxes in the reference frame), and a set of M bounding boxes (representing the boxes in the next frame) and the output will be an NxM matrix, with the ```[i, j]```th entry correspoding to bounding box ```i``` in the reference frame's IoU with bounding box ```j``` of the next frame.\n",
    "\n",
    "For this question, please submit the matrix of IoUs between the boxes on the 124th and 125th frames, as well as your code to the writeup PDF, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        boxes1: Nx4 ndarray, representing N bounding boxes coordinates\n",
    "        boxes2: Nx4 ndarray, representing N bounding boxes coordinates\n",
    "    Output: \n",
    "        iou_mat: NxM ndarray, with iou_mat[i, j] = iou(boxes1[i], boxes2[j])\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.3 Matching with the Hungarian Algorithm (10 pts)\n",
    "\n",
    "Given a matrix of similarities, the next step in tracking by detection is to find the bounding box that corresponds most to the previous frame. In essence, the idea is to find the bounding box that has the closest IoU to a bounding box in a previous frame. The challenging part is to remove this bounding box from contention for other matches. This problem is known as the optimal cost assignment problem. The Hungarian algorithm is the most commonly used method to solve this problem, and is implemented in ```scipy.optimize.linear_sum_assignment```. Below, you will implement a function ```compute_assignment``` that will produce such a matching. \n",
    "\n",
    "Some notes to keep in mind: ```scipy```'s implementation uses costs, so you will need to use the negative of the similarities you computed in the previous part. Additionally, since the IoU of a bounding box with itself is 1, you will need to set the diagonal entries of the cost matrix to some high value so they are not picked. Finally, it's likely there will be matches with very low IoU scores. Please use the ```threshold``` parameter to filter out any matches that are below ```threshold``` IoU.\n",
    "\n",
    "Please submit your code for this section to the writeup PDF, along with the output run on the IOU matrix computed between the 124th and 125th images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_assignment(iou_matrix, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Given an input matrix of IoUs, uses the Hungarian algorithm to compute a matching.\n",
    "    \n",
    "    Args:\n",
    "        iou_matrix: NxM matrix of IoUs of bounding boxes between two frames.\n",
    "        threshold: float value representing minimum value of IoU to be considered as a candidate for matching.\n",
    "\n",
    "    Returns:\n",
    "        box1_ind, box2_ind : a set of indices into box1 (bboxes of ref frame) and corresponding \n",
    "        indices into box2 representing the optimal assignment.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.4 Putting it all together (20 pts)\n",
    "\n",
    "Now, you will put all the pieces together that you implemented above to create a full tracking system. You will maintain a set of tracks throughout the video. At the beginning, no tracks will exist, only detections. In each successive frame, you will read in the detections for that frame and associate the new detections to the previous ones, and create candidate tracks. If a candidate track has persisted for P frames, you will add it to the list of tracks. If a track has not had a match in K frames, you will remove it from the list of tracks. For each frame, you will render the bounding boxes corresponding to every track. Once a track is removed from the list, do not render its bounding box. \n",
    "\n",
    "The hyperparameters you will use for the tracker are:\n",
    "\n",
    "P: number of frames a candidate track must exist before it is added to the list of tracks\n",
    "K: number of frames a track must have no match for before it is removed from the list of tracks\n",
    "iou_thresh: threshold to be used for whether a match is strong enough to be added to a track.\n",
    "\n",
    "\n",
    "For your submission for this part, please submit 10 images (with bounding boxes) from successive frames at any point in the video. Bounding boxes belonging to the same track should be the same color. Please also submit your code to the writeup PDF. \n",
    "\n",
    "Please note that the output will NOT be perfect! There should be ID switches and the tracker, even if implemented correctly, will likely fail in several frames. This should hopefully demonstrate to you that tracking is a tough problem and show why it is a hot research area today :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tracker(images, boxes, P, K, iou_thresh):\n",
    "    \"\"\"\n",
    "    Runs the entire tracking pipeline. \n",
    "\n",
    "    Args:\n",
    "        images: numpy array of N images. \n",
    "        boxes: list of bounding boxes for \n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a14d3cb3a8a01d6659ca21286c75f37ed68cd344cf818a36ad292d43e0de27a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
