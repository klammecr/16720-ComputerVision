{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf5e700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e779e94",
   "metadata": {},
   "source": [
    "## Appendix: Neural Network Overview\n",
    "Deep learning has quickly become one of the most applied machine learning techniques in computer vision. Convolutional neural networks have been applied to many different computer vision problems such as image classification, recognition, and segmentation with great success. In this assignment, you will first implement a fully connected feed forward neural network for hand written character classification. Then in the second part, you will implement a system to locate characters in an image, which you can then classify with your deep network. The end result will be a system that, given an image of hand written text, will output the text contained in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573282b",
   "metadata": {},
   "source": [
    "### Basic Use\n",
    "Here we will give a brief overview of the math for a single hidden layer feed forward network. For a more detailed look at the math and derivation, please see the class slides.\n",
    "\n",
    "A fully-connected network $\\textbf{f}$, for classification, applies a series of linear and non-linear functions to an input data vector $\\textbf{x}$ of size $N\\times 1$ to produce an output vector $\\textbf{f}(\\textbf{x})$ of size $C\\times 1$, where each element $i$ of the output vector represents the probability of $\\textbf{x}$ belonging to the class $i$. Since the data samples are of dimensionality $N$, this means the input layer has $N$ input units. To compute the value of the output units, we must first compute the values of all the hidden layers. The first hidden layer *pre-activation* $\\textbf{a}^{(1)}(\\textbf{x})$ is given by\n",
    "\n",
    "$$\\textbf{a}^{(1)}(\\textbf{x}) = \\textbf{W}^{(1)}\\textbf{x} + \\textbf{b}^{(1)}$$\n",
    "\n",
    "Then the *post-activation* values of the first hidden layer $\\textbf{h}^{(1)}(\\textbf{x})$ are computed by applying a non-linear activation function $\\textbf{g}$ to the *pre-activation* values\n",
    "\n",
    "$$\\textbf{h}^{(1)}(\\textbf{x}) = \\textbf{g}(\\textbf{a}^{(1)}(\\textbf{x})) = \\textbf{g}(\\textbf{W}^{(1)}\\textbf{x} + \\textbf{b}^{(1)})$$\n",
    "\n",
    "Subsequent hidden layer ($1 < t \\leq T$) pre- and post activations are given by:\n",
    "\n",
    "$$\\textbf{a}^{(t)}(\\textbf{x}) = \\textbf{W}^{(t)}\\textbf{h}^{(t-1)} + \\textbf{b}^{(t)}$$\n",
    "\n",
    "$$\\textbf{h}^{(t)}(\\textbf{x}) = \\textbf{g}(\\textbf{a}^{(t)}(\\textbf{x}))$$\n",
    "\n",
    "The output layer *pre-activations* $\\textbf{a}^{(T)}(\\textbf{x})$ are computed in a similar way\n",
    "\n",
    "$$\\textbf{a}^{(T)}(\\textbf{x}) = \\textbf{W}^{(T)}\\textbf{h}^{(T-1)}(\\textbf{x}) + \\textbf{b}^{(T)}$$\n",
    "\n",
    "and finally the \\emph{post-activation} values of the output layer are computed with\n",
    "$$\\textbf{f}(\\textbf{x}) = \\textbf{o}(\\textbf{a}^{(T)}(\\textbf{x})) = \\textbf{o}(\\textbf{W}^{(T)}\\textbf{h}^{(T-1)}(\\textbf{x}) + \\textbf{b}^{(T)})$$\n",
    "\n",
    "where $\\textbf{o}$ is the output activation function. Please note the difference between $\\textbf{g}$ and $\\textbf{o}$! \n",
    "For this assignment, we will be using the sigmoid activation function for the hidden layer, so:\n",
    "$$\\textbf{g}(y) = \\frac{1}{1+\\exp(-y)}$$\n",
    "where when $\\textbf{g}$ is applied to a vector, it is applied element wise across the vector.\n",
    "\n",
    "Since we are using this deep network for classification, a common output activation function to use is the softmax function. This will allow us to turn the real value, possibly negative values of $\\textbf{a}^{(T)}(\\textbf{x})$ into a set of probabilities (vector of positive numbers that sum to 1). Letting $\\textbf{x}_i$ denote the $i^{th}$ element of the vector $\\textbf{x}$, the softmax function is defined as:\n",
    "$$\\textbf{o}_i(\\textbf{y}) = \\frac{\\exp(\\textbf{y}_i)}{\\sum_j \\exp(\\textbf{y}_j)}$$\n",
    "\n",
    "![](figures/letter_montage.jpg)\n",
    "<center>Samples from NIST Special 19  dataset</center>\n",
    "\n",
    "\n",
    "Gradient descent is an iterative optimisation algorithm, used to find the local optima. To find the local minima, we start at a point on the function and move in the direction of negative gradient (steepest descent) till some stopping criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946fa58",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "The update equation for a general weight $W^{(t)}_{ij}$ and bias $b^{(t)}_i$ is\n",
    "$$\n",
    "W^{(t)}_{ij} = W^{(t)}_{ij} - \\alpha*\\frac{\\partial L_{\\textbf{f}}}{\\partial W^{(t)}_{ij}}(\\textbf{x})\\hspace{1cm}\n",
    "b^{(t)}_{i} = b^{(t)}_{i} - \\alpha*\\frac{\\partial L_{\\textbf{f}}}{\\partial b^{(t)}_{i}}(\\textbf{x})\n",
    "$$\n",
    "$\\alpha$ is the learning rate. Please refer to the back-propagation slides for more details on how to derive the gradients. Note that here we are using softmax loss (which is different from the least square loss in the slides)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3c4f3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]  Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. 2010. http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf.\n",
    "\n",
    "[2]  P. J. Grother. Nist special database 19 â€“ handprinted forms and characters database. https://www.nist.gov/srd/nist-special-database-19, 1995."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
