{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXfZbMP-dHSx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "d5wmWkIEdHS0",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c4a28c939dd2131a33d2c8b68c118b4d",
          "grade": false,
          "grade_id": "cell-6c9f551ea02c7bf9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
        "\n",
        "#                                    16720 (B) Neural Networks for Recognition - Assignment 3\n",
        "\n",
        "     Instructor: Kris Kitani                       TAs: Arka, Jinkun, Rawal, Rohan, Sheng-Yu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "j9eRAGCZdHS2",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d3b18c2962769d4c7f5fa2046057f9f2",
          "grade": false,
          "grade_id": "cell-372b7e8bfa64e2d2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## Q1 Implementing a Fully Connected Network (75 points)\n",
        "\n",
        "**Please include all the answers to the write-up questions to HW3:PDF**. Questions are indicated either the \"write-up\" or \"auto-grader\" tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8947_jqAdHS4",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "10a52a255fcde49ea866eed7aab3f8fd",
          "grade": false,
          "grade_id": "cell-2d56327246e23156",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Do Not Modify\n",
        "# Do Not Import ANY other packages\n",
        "import numpy as np\n",
        "\n",
        "# use for a \"no activation\" layer\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def linear_deriv(post_act):\n",
        "    return np.ones_like(post_act)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_deriv(post_act):\n",
        "    return 1-post_act**2\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    return (x > 0).astype(np.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xiOIKyjodHS6",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "79ff1a768bc92c35caeaa705c9304131",
          "grade": false,
          "grade_id": "cell-cf0ebdeb56cae121",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Q1.1 Network Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "FzeJ8rE7dHS7",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "12e71c9c0c03aee4fd5317e95475fd79",
          "grade": false,
          "grade_id": "cell-810587ffe33a3598",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Q1.1.1 (3 points, write-up)\n",
        "Why is it not a good idea to initialize a network with all zeros? If you imagine that every layer has weights and biases, what can a zero-initialized network output after training?\n",
        "\n",
        "<font color=\"red\">**Please include your answer to HW3:PDF**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Q0R8zkkvdHS8",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "927e81c1fd404da2ff3d44df4620daac",
          "grade": false,
          "grade_id": "cell-72609a649db69760",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Q1.1.2 (3 points, auto-grader)\n",
        "Implement `initialize_weights` below to initialize neural network weights with [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), where $Var[w] = \\frac{2}{n_{in}+ n_{out}} $ and $n$ is the dimensionality of the vectors. Please use an **uniform distribution** to sample random numbers (see eq 16 in [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)), we recommend using np.random.uniform()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "deletable": false,
        "id": "yLjGLktodHS9",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "58275718a53d1e6bd9930a48cf0587bb",
          "grade": false,
          "grade_id": "cell-04dbce39e616d009",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def initialize_weights(in_size: int, out_size: int, params: dict, name: str='' ):\n",
        "    '''\n",
        "    Initialize the weights W and b for a linear layer Y = XW + b\n",
        "    \n",
        "    [input]\n",
        "    * in_size -- the feature dimension of the input\n",
        "    * out_size -- the feature dimension of the output\n",
        "    * params -- a dictionary containing parameters\n",
        "    * name -- name of the layer\n",
        "    \n",
        "    HINTS:\n",
        "    (1) b should be a 1D array, not a 2D array with a singleton dimension\n",
        "    '''\n",
        "    init_value = (6 / (in_size + out_size)) ** .5\n",
        "    W, b = np.random.uniform(-init_value, init_value, size=(in_size, out_size)), np.zeros((out_size,))\n",
        "\n",
        "    params['W' + name] = W\n",
        "    params['b' + name] = b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NbaGK5ZldHS-",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "629cf17bed61b21fc736561554da254d",
          "grade": true,
          "grade_id": "q1_1_2",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "params = {}\n",
        "initialize_weights(2, 25, params, 'layer1')\n",
        "initialize_weights(25, 4, params, 'output')\n",
        "assert(params['Wlayer1'].shape == (2, 25))\n",
        "assert(params['blayer1'].shape == (25,))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NevXhUmDdHTA",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a434d088469e76516c98458de89f8bf9",
          "grade": false,
          "grade_id": "cell-50fa916b36662a34",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Q1.1.3 (2 points, write-up)\n",
        "Why is it a good practice to initialize the parameters using random numbers? Explain the intuition behind scaling the initializations depending on layer size (see near Fig 6 in [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf))?\n",
        "\n",
        "<font color=\"red\">**Please include your answer to HW3:PDF**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ulF8USqvdHTB",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bbbf3ef2329ab0ab1f85d2a42bb8ea71",
          "grade": false,
          "grade_id": "cell-e1c75223ce84f014",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Q1.2 Forward Propagation\n",
        "\n",
        "Please refer to `appendix.jpynb` for the forward propagation equations. We will be implementing the forward propagation in code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "OVoV3rhQdHTB",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3aa4fd84feb6b3246703f07a9aa69e77",
          "grade": false,
          "grade_id": "cell-edc97acfe64d2a32",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Q1.2.1 (12 points, auto-grader)\n",
        "Implement `sigmoid`, along with `forward` propagation for a single layer with an activation function, namely\n",
        "$y = \\sigma(X W + b)$, returning the output and intermediate results for an $N \\times D$ dimension input $X$, with examples along the rows, data dimensions along the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "deletable": false,
        "id": "8yuQ8gh7dHTC",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "80e796be52b7c3367508fb5e5fa498c0",
          "grade": false,
          "grade_id": "cell-9d629c29b34231c5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def sigmoid(x: np.ndarray):\n",
        "    '''\n",
        "    A sigmoid activation function\n",
        "    \n",
        "    [input]\n",
        "    * X -- input data [N x D]\n",
        "    \n",
        "    [output]\n",
        "    * res -- output after the sigmoid function\n",
        "    '''\n",
        "    return 1 / (1 + np.exp(-(x + 1e-6)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "deletable": false,
        "id": "4AAdzqCddHTE",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "057981ca81468558e373b7b99d89e4ef",
          "grade": false,
          "grade_id": "cell-8a7b5e6897a9d712",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def forward(X: np.ndarray, params: dict, name: str='',\n",
        "            activation: callable=sigmoid):\n",
        "    \"\"\"\n",
        "    Do a forward pass\n",
        "\n",
        "    [input]\n",
        "    * X -- input data [N x D]\n",
        "    * params -- a dictionary containing parameters\n",
        "    * name -- name of the layer\n",
        "    * activation -- the activation function (default is sigmoid)\n",
        "    \n",
        "    [output]\n",
        "    * post_act -- output after a linear layer and activation\n",
        "    \"\"\"\n",
        "    pre_act, post_act = None, None\n",
        "    # get the layer parameters\n",
        "    W = params['W' + name]\n",
        "    b = params['b' + name]\n",
        "    \n",
        "    ## compute pre_act using X, W and b.\n",
        "    ## compute post_act using pre_act.\n",
        "    pre_act  = X @ W + b\n",
        "    post_act = activation(pre_act)\n",
        "\n",
        "    # store the pre-activation and post-activation values\n",
        "    # these will be important in backprop\n",
        "    params['cache_' + name] = (X, pre_act, post_act)\n",
        "\n",
        "    return post_act"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "asBYYYGrdHTF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8270a786b76230cf0608a396401307d4",
          "grade": true,
          "grade_id": "q1_2_1_forward",
          "locked": true,
          "points": 9,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "params = {'Wlayer1': np.random.rand(10, 25), 'blayer1': np.random.rand(25,)}\n",
        "X = np.random.rand(3, 10)\n",
        "y = forward(X, params, 'layer1')\n",
        "assert 'cache_layer1' in params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "asi5saYSdHTG",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b1efb13c6b9d712219e8635c09b571a7",
          "grade": false,
          "grade_id": "cell-e82fe9a6f4b520d6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Q1.2.2 (5 points, auto-grader)\n",
        "Implement the `softmax` function. Please implement a numerically stable computation of softmax using Theory:Q2. Hint, translate the input using the maximum element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "deletable": false,
        "id": "s7M70BnYdHTG",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4545cdaffdf5e1a0488899cf65ee514a",
          "grade": false,
          "grade_id": "cell-e41cfc344ff8bdef",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def softmax(X: np.ndarray):\n",
        "    \"\"\"\n",
        "    A softmax function.\n",
        "    \n",
        "    [input]\n",
        "    * X -- input data [N x D]\n",
        "    \n",
        "    [output]\n",
        "    * res -- values after softmax\n",
        "    \"\"\"\n",
        "    exp_res = np.exp(X  - np.max(X, axis = 1).reshape(X.shape[0], 1))\n",
        "    res     = exp_res / np.sum(exp_res, axis = 1).reshape(X.shape[0], 1)\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZfNX_K4_7bd",
        "outputId": "66c7edf2-97c4-4872-fb20-54548ab76789"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.09003057, 0.24472847, 0.66524096],\n",
              "       [0.09003057, 0.24472847, 0.66524096]])"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = np.array([[1, 2, 3], [1, 2, 3]])\n",
        "softmax(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "L9cUrSKTdHTH",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3ab6c7d80f21f9c99cf46dbc134250ae",
          "grade": true,
          "grade_id": "q1_2_2",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "TVOuyq52dHTI",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "df923e1e58fe4b4f853cf175ac6612f9",
          "grade": false,
          "grade_id": "cell-ca7affc876fccf2f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Q1.2.3 (5 points, auto-grader)\n",
        "Implement `compute_loss_and_acc` to compute the accuracy of a set of labels, along with the scalar loss across the data.  The loss function generally used for classification is the cross-entropy loss.\n",
        "\n",
        "$$L_{\\textbf{f}}(\\textbf{D}) = - \\sum_{(\\textbf{x}, \\textbf{y})\\in \\textbf{D}}\\textbf{y}\\cdot\\log(\\textbf{f}(\\textbf{x}))$$\n",
        "Here $\\textbf{D}$ is the full training dataset of data samples $\\textbf{x}$ ($N\\times 1$ vectors, N = dimensionality of data) and labels $\\textbf{y}$ ($C\\times 1$ one-hot vectors, C = number of classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "deletable": false,
        "id": "0QiDHOtMdHTI",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d377251adf8aa34e97eafce05f017cf5",
          "grade": false,
          "grade_id": "cell-e8d3b0354b03c98c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def compute_loss_and_acc(y: np.ndarray, probs: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute total loss and accuracy\n",
        "    \n",
        "    [input]\n",
        "    * y -- one hot labels [N x C]\n",
        "    * probs -- class probabities [N x C]\n",
        "    \n",
        "    [output]\n",
        "    * loss -- cross-entropy loss\n",
        "    * acc -- accuracy\n",
        "    \"\"\"\n",
        "    loss = -np.sum(y * np.log(probs + 1e-20))\n",
        "    acc  = np.sum(np.argmax(probs, axis = 1) == np.argmax(y, axis = 1)) / y.shape[0]\n",
        "\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wTyHrWiUGzn",
        "outputId": "32ba3318-aa65-431e-eb53-49d9ecd3c992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 5.016150474784366\n",
            "Accuracy: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "probs = np.array([[.51, .49, 0], [.98, 0.02, 0.0], [.65, .33, .02]])\n",
        "y     = np.array([[1, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
        "loss, acc = compute_loss_and_acc(y, probs)\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Accuracy: {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2MTaZklRdHTK",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d27e1c1d859714e4fc20557d557f7e74",
          "grade": true,
          "grade_id": "q1_2_3",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7VEBpFxmdHTK",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "958acb8267ab41402ed6ed648f137383",
          "grade": false,
          "grade_id": "cell-32109cdbdb894d52",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Q1.3 Backwards Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b4LSkj7AdHTK",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "56fbf5998a62df67acc2890a35e3280c",
          "grade": false,
          "grade_id": "cell-f50e629f2d59ad01",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Q1.3.1 (10 points, auto-grader)\n",
        "Compute back-propagation for a single layer, given the original weights, the appropriate intermediate results, and given gradient with respect to the loss. You should return the gradient with respect to $X$ so you can feed it into the next layer. As a sanity check, your gradients should be the same dimensions as the original objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "deletable": false,
        "id": "DWRbD4GMdHTL",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "495f65a53090819a1c4ac3176b95c89c",
          "grade": false,
          "grade_id": "cell-05a2aee350a000f3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def sigmoid_deriv(post_act: np.ndarray):\n",
        "    \"\"\"\n",
        "    Derivative of sigmoid.\n",
        "    \n",
        "    we give this to you because you proved it\n",
        "    it's a function of post_act\n",
        "    \"\"\"\n",
        "    res = post_act*(1.0-post_act)\n",
        "    return res\n",
        "\n",
        "\n",
        "def backwards(delta: np.ndarray, params: dict, name: str='',\n",
        "              activation_deriv: callable=sigmoid_deriv):\n",
        "    \"\"\"\n",
        "    Do a backwards pass\n",
        "\n",
        "    [input]\n",
        "    * delta -- errors to backprop\n",
        "    * params -- a dictionary containing parameters\n",
        "    * name -- name of the layer\n",
        "    * activation_deriv -- the derivative of the activation_func\n",
        "    \n",
        "    [output]\n",
        "    * grad_X -- gradient w.r.t X\n",
        "    \"\"\"\n",
        "    grad_X, grad_W, grad_b = None, None, None\n",
        "    # everything you may need for this layer\n",
        "    W = params['W' + name]\n",
        "    b = params['b' + name]\n",
        "    X, pre_act, post_act = params['cache_' + name]\n",
        "\n",
        "    # DEBUG: Printing shapes\n",
        "    # print(f\"Pre activation shape: {pre_act.shape}\")\n",
        "    # print(f\"Post activation shape: {post_act.shape}\")\n",
        "    # print(f\"X shape: {X.shape}\")\n",
        "    # print(f\"W shape: {W.shape}\")\n",
        "    # print(f\"Delta shape: {delta.shape}\")\n",
        "\n",
        "    # Do the derivative through activation first\n",
        "    # then compute the derivative W,b, and X\n",
        "\n",
        "    act_deriv = activation_deriv(post_act)\n",
        "    # Calculate the error by multiplying the activation derivative (signal) \n",
        "    # times the error between the prediction and the label\n",
        "    errors    = act_deriv * delta\n",
        "\n",
        "    \n",
        "    # The gradient wrt X is the weighted errors\n",
        "    grad_X = errors @ W.T\n",
        "    # print(f\"Gradient wrt X Shape: {grad_X.shape}\")\n",
        "\n",
        "    # The gradient wrt W is weighted by the x values\n",
        "    grad_W = (X.T @ errors)\n",
        "\n",
        "    # Just the errors, average it over the batch size\n",
        "    grad_b = np.sum(errors, axis=0) / errors.shape[0]\n",
        "\n",
        "    # store the gradients\n",
        "    params['grad_W' + name] = grad_W\n",
        "    params['grad_b' + name] = grad_b\n",
        "\n",
        "    return grad_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "YsB2tGszdHTL",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9b35b71d3a6a6929ebbfa9fd128a7767",
          "grade": true,
          "grade_id": "q1_3_1",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "d757bc50-e46e-4780-efa7-e9a6a4283c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# we use random values to test your implementation \n",
        "# independent of previous questions\n",
        "n, c1, c2 = 5, 40, 20 \n",
        "delta = np.random.rand(n, c2)\n",
        "name = 'layer1'\n",
        "params = {\n",
        "    'W'+name: np.random.rand(c1, c2),\n",
        "    'b'+name: np.random.rand(c2),\n",
        "    'cache_'+name: (np.random.rand(n, c1), \n",
        "                     np.random.rand(n, c2), \n",
        "                     np.random.rand(n, c2))\n",
        "}\n",
        "print()\n",
        "grad = backwards(delta, params, name, tanh_deriv)\n",
        "\n",
        "assert 'grad_W' + name in params\n",
        "assert 'grad_b' + name in params\n",
        "\n",
        "assert params['grad_W'+name].shape == params['W'+name].shape\n",
        "assert params['grad_b'+name].shape == params['b'+name].shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "pU2XXC-NdHTM",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "41d97d29c9a5d4f9c9df877c531a3838",
          "grade": false,
          "grade_id": "cell-4fbc1b9ed142d412",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Q1.4 Convolutional Layer (10 points)\n",
        "\n",
        "For now we have worked with linear layer in fully-connected networks. In practice, convolutional layers are commonly used to extract image feature. You will implement the forward and backawad propagation for convolutional layer in this subsection. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "iCN4W-14dHTN",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bea742d6edacc6c63678c9527f3f9012",
          "grade": false,
          "grade_id": "cell-56852c776e6811a6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Q1.4.1 (5 points, auto-grader)\n",
        "Similar to Q1.2.1, implement `conv_forward` for a single convolutional layer with zero paddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "deletable": false,
        "id": "U0xfnGPPdHTN",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "be891bcc9486e7726997d016f0ddbe4d",
          "grade": false,
          "grade_id": "cell-5e3595068856df10",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def pad_img(img, pad):\n",
        "  return np.stack([np.pad(Xc, pad) for Xc in img])\n",
        "\n",
        "def get_empty_out(X, filt, stride, pad):\n",
        "    F, _, HH, WW = filt.shape\n",
        "    out_rows = int(((X.shape[2] + 2 * pad - HH)/stride) + 1)\n",
        "    out_cols = int(((X.shape[3] + 2 * pad - WW)/stride) + 1)\n",
        "    res = np.zeros((X.shape[0], F, out_rows, out_cols))\n",
        "    return res, out_rows, out_cols\n",
        "\n",
        "\n",
        "def conv_forward(X: np.ndarray, params: dict, name: str='',\n",
        "            stride: int=1, pad: int=0):\n",
        "    \"\"\"\n",
        "    Do a forward pass for a convolutional layer\n",
        "\n",
        "    [input]\n",
        "    * X -- input data [N x C x H x W]\n",
        "    * params -- a dictionary containing parameters\n",
        "    * name -- name of the layer\n",
        "    * stride, pad -- convolution parameters\n",
        "    \n",
        "    [output]\n",
        "    * res -- output after a convolutional layer\n",
        "    \"\"\"\n",
        "    res = None\n",
        "    # get the layer parameters\n",
        "    w = params['W' + name] # Conv Filter weights [F x C x HH x WW]\n",
        "    b = params['b' + name] # Biases [F]\n",
        "\n",
        "    # Debug Dimensions\n",
        "    print(f\"X dimensions: {X.shape}\")\n",
        "      \n",
        "    # Filter Details\n",
        "    F, _, HH, WW = w.shape\n",
        "\n",
        "    # Get empty output array\n",
        "    res, out_rows, out_cols = get_empty_out(X, w, stride, pad)\n",
        "\n",
        "    # Iterate over the batch\n",
        "    for img_num in range(X.shape[0]):\n",
        "      \n",
        "      # Pad the image\n",
        "      X_pad = pad_img(X[img_num], pad)\n",
        "      # print(X_pad.shape)\n",
        "\n",
        "      for i in range(out_rows):\n",
        "        for j in range(out_cols):\n",
        "            for filt_num, filt in enumerate(w):\n",
        "              # Compute the idx for the X matrix\n",
        "              i_start = stride * i\n",
        "              i_end   = i_start + HH\n",
        "              j_start = stride * j\n",
        "              j_end   = j_start + WW\n",
        "\n",
        "              # Calculate the index in the padding\n",
        "              X_patch = X_pad[:, i_start:i_end, j_start:j_end]\n",
        "\n",
        "              # Debug the shape of the patch and the filter\n",
        "              # print(f\"X Patch Shape: {X_patch.shape}\")\n",
        "              # print(f\"Filt Shape: {filt.shape}\")\n",
        "\n",
        "              # Convolve each filter and add the bias\n",
        "              conv_res = np.dot(X_patch.flatten(), filt.flatten()) + b[filt_num]\n",
        "              # print(f\"({img_num}, {c}, {i//stride}, {j//stride})\")\n",
        "              res[img_num, filt_num, i, j] += conv_res\n",
        "            \n",
        "    # store the input and convolution parameters\n",
        "    # these will be important in backprop\n",
        "    params['cache_' + name] = (X, stride, pad)\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "N2aNqWywdHTN",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b3bdddcb79670ea17ad0c5e167ea7e30",
          "grade": true,
          "grade_id": "q1_4_1",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "df47de10-1450-4d78-90ea-1760effee7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X dimensions: (2, 3, 4, 4)\n",
            "[[[[-3.87559808e-09 -3.59587783e-09]\n",
            "   [-2.44387191e-09  4.71107839e-09]]\n",
            "\n",
            "  [[ 2.99227090e-09  2.02061101e-09]\n",
            "   [-5.81523746e-10 -4.67795361e-09]]\n",
            "\n",
            "  [[-1.39860123e-10 -2.36290021e-09]\n",
            "   [ 1.28082456e-09 -4.06698553e-09]]]\n",
            "\n",
            "\n",
            " [[[-4.83253570e-09 -3.30143513e-09]\n",
            "   [ 1.60471125e-09  1.10418341e-11]]\n",
            "\n",
            "  [[ 1.96908356e-09  2.24880392e-09]\n",
            "   [ 3.40080963e-09  5.55759994e-10]]\n",
            "\n",
            "  [[-1.22929755e-09 -2.20095719e-09]\n",
            "   [-4.80309170e-09  1.10047838e-09]]]]\n",
            "1.01418245052412e-09\n"
          ]
        }
      ],
      "source": [
        "x_shape = np.array((2, 3, 4, 4))\n",
        "w_shape = np.array((3, 3, 4, 4))\n",
        "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape), dtype=np.float64).reshape(*x_shape)\n",
        "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape), dtype=np.float64).reshape(*w_shape)\n",
        "b = np.linspace(-0.1, 0.2, num=3, dtype=np.float64)\n",
        "\n",
        "params = {'WConv_layer1': w, 'bConv_layer1': b}\n",
        "y = conv_forward(np.array(x), params, 'Conv_layer1', stride=2, pad=1)\n",
        "assert 'cache_Conv_layer1' in params\n",
        "\n",
        "\n",
        "y_ref = np.array([[[[-0.08759809, -0.10987781],\n",
        "                              [-0.18387192, -0.2109216 ]],\n",
        "                             [[ 0.21027089,  0.21661097],\n",
        "                              [ 0.22847626,  0.23004637]],\n",
        "                             [[ 0.50813986,  0.54309974],\n",
        "                              [ 0.64082444,  0.67101435]]],\n",
        "                            [[[-0.98053589, -1.03143541],\n",
        "                              [-1.19128892, -1.24695841]],\n",
        "                             [[ 0.69108355,  0.66880383],\n",
        "                              [ 0.59480972,  0.56776003]],\n",
        "                             [[ 2.36270298,  2.36904306],\n",
        "                              [ 2.38090835,  2.38247847]]]], \n",
        "            )\n",
        "# print(y_ref.shape)\n",
        "assert y.shape == y_ref.shape\n",
        "print(y_ref - y)\n",
        "\n",
        "max_diff = np.max(np.abs((y_ref - y)))\n",
        "base = (np.abs(y_ref) + np.abs(y)).clip(np.finfo(float).eps).max()\n",
        "print(max_diff/base) # the difference should be less than 1e-8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "lzkOTHUudHTN",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c33dfa5415a8379ded48c910896ebc02",
          "grade": false,
          "grade_id": "cell-984447a667304ac8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Q1.4.2 (5 points, auto-grader)\n",
        "Implement `conv_backword` for a single convolutional layer with zero paddings.\n",
        "Compute back-propagation for a single convolutional layer, given the original weights, the cached input, and given gradient with respect to the loss. Similar to Q1.3.1, you should return the gradient with respect to $X$ so you can feed it into the next layer. As a sanity check, your gradients should be the same dimensions as the original objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "deletable": false,
        "id": "tN-N3sDFdHTO",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "058aa9f5d5dd5f8d6d878e39b2ff6ad1",
          "grade": false,
          "grade_id": "cell-cc53a61e4f74a964",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def conv_backward(delta: np.ndarray, params: dict, name: str=''):\n",
        "    \"\"\"\n",
        "    Do a backwards pass for a convolutional layer\n",
        "\n",
        "    [input]\n",
        "    * delta -- errors to backprop\n",
        "    * params -- a dictionary containing parameters\n",
        "    * name -- name of the layer\n",
        "    \n",
        "    [output]\n",
        "    * grad_X -- gradient w.r.t X\n",
        "    \"\"\"\n",
        "    grad_X, grad_W, grad_b = None, None, None\n",
        "    # everything you may need for this layer\n",
        "    W = params['W' + name]\n",
        "    b = params['b' + name]\n",
        "    X, stride, pad = params['cache_' + name]\n",
        "\n",
        "    # Filter Details\n",
        "    F, _, HH, WW = w.shape\n",
        "\n",
        "    # Get empty output array\n",
        "    _, out_rows, out_cols = get_empty_out(X, W, stride, pad)\n",
        "    print(f\"Out Rows: {out_rows}\")\n",
        "    print(f\"Out Cols: {out_cols}\")\n",
        "\n",
        "    # Instaniate arrays for grads of X, W, b\n",
        "    grad_X = np.zeros((X.shape[0], X.shape[1], X.shape[2] + 2*pad, X.shape[3] + 2*pad))\n",
        "    grad_W = np.zeros_like(W)\n",
        "    grad_b = np.zeros_like(b)\n",
        "\n",
        "    # Iterate over the batch\n",
        "    for img_num in range(X.shape[0]):\n",
        "      \n",
        "      # Pad the image\n",
        "      X_pad = pad_img(X[img_num], pad)\n",
        "\n",
        "      for i in range(out_rows):\n",
        "        for j in range(out_cols):\n",
        "            for filt_num, filt in enumerate(w):\n",
        "              # Compute the idx for the X matrix\n",
        "              i_start = stride * i\n",
        "              i_end   = i_start + HH\n",
        "              j_start = stride * j\n",
        "              j_end   = j_start + WW\n",
        "\n",
        "              # Find the patch of X used for the original conv.\n",
        "              X_patch = X_pad[:, i_start:i_end, j_start:j_end]\n",
        "\n",
        "              # For X, weights times delta, the weights are weighted by the error\n",
        "              try:\n",
        "                grad_X[img_num, :, i_start:i_end, j_start:j_end] += filt * delta[img_num, filt_num, i, j]\n",
        "              except:\n",
        "                print(f\"{i_start} {i_end}\")\n",
        "\n",
        "              # For W, X times delta. Each slice ij times delta (the X value of that slice times delta)\n",
        "              grad_W[filt_num] += X_patch * delta[img_num, filt_num, i, j]\n",
        "\n",
        "              # For b, just sum up the delta\n",
        "              grad_b[filt_num] += delta[img_num, filt_num, i, j]\n",
        "\n",
        "    # store the gradients\n",
        "    params['grad_W' + name] = grad_W\n",
        "    params['grad_b' + name] = grad_b\n",
        "\n",
        "    # Get rid of the padding\n",
        "    return grad_X[:, :, pad:-pad, pad:-pad]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "QunEBkPVdHTO",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3f1df3bc75ca1df7a721d8b116df16bb",
          "grade": true,
          "grade_id": "q1_4_2",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "c5876bb4-7b81-43f9-a338-1792f6e995be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X dimensions: (5, 4, 16, 16)\n",
            "Out Rows: 8\n",
            "Out Cols: 8\n"
          ]
        }
      ],
      "source": [
        "x = np.random.rand(5, 4, 16, 16)\n",
        "w = np.random.rand(8, 4, 7, 7)\n",
        "b = np.random.rand(8,)\n",
        "dout = np.random.rand(5, 8, 8, 8)\n",
        "\n",
        "params = {'WConv_layer1': w, 'bConv_layer1': b}\n",
        "y = conv_forward(x, params, 'Conv_layer1', stride=2, pad=3)\n",
        "dx = conv_backward(dout, params, 'Conv_layer1')\n",
        "assert x.shape == dx.shape\n",
        "assert params['grad_WConv_layer1'].shape == params['WConv_layer1'].shape\n",
        "assert params['grad_bConv_layer1'].shape == params['bConv_layer1'].shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5WumLD9WdHTP",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a1ec406ff158fc34e56353b9584f4719",
          "grade": false,
          "grade_id": "cell-1ca2e358a302f02f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Q1.5 The Training Loop\n",
        "You usually see gradient descent in three forms: \"normal\", \"stochastic\" and \"batch\". \"Normal\" gradient descent aggregates the updates for the entire dataset before changing the weights. Stochastic gradient descent applies updates after every single data example. Batch gradient descent is a compromise, where random subsets of the full dataset are evaluated before applying the gradient update. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "H1c6iaXddHTP",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5e510393e089402c2b32c5d81eb2bc33",
          "grade": false,
          "grade_id": "cell-683134f646db0287",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Q1.5.1 (10 points, auto-grader)\n",
        "Write a training loop that generates random batches, iterates over them for many iterations, does forward and backward propagation, and applies a gradient update step. Specifically, implement `get_random_batches` and `train` functions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "deletable": false,
        "id": "lgYtEZlIdHTQ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0f1b9f89609b51466428af088518dc94",
          "grade": false,
          "grade_id": "cell-1b13b59b8a93b801",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_random_batches(x: np.ndarray, y: np.ndarray, batch_size: int) -> list:\n",
        "    \"\"\"\n",
        "    Split x and y into random batches\n",
        "    \n",
        "    [input]\n",
        "    * x -- training samples\n",
        "    * y -- training lables\n",
        "    * batch_size -- batch size\n",
        "    \n",
        "    [output]\n",
        "    * batches -- a list of [(batch1_x,batch1_y)...]\n",
        "    \"\"\"\n",
        "    batches = []\n",
        "    shuffled_idxs = np.random.permutation(y.shape[0])\n",
        "    shuffled_x    = x[shuffled_idxs]\n",
        "    shuffled_y    = y[shuffled_idxs]\n",
        "    for i in range(0, len(y), batch_size):\n",
        "      # Create the batch of X\n",
        "      batches.append((shuffled_x[i:i+batch_size], shuffled_y[i:i+batch_size]))\n",
        "    # print(batches[0])\n",
        "    return batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "FUoV-PeadHTQ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "446651dc1f57701ff98d71c44fe3d7f4",
          "grade": true,
          "grade_id": "q1_5_1_random_batches",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "n, c1, c2 = 20, 100, 5\n",
        "batch_size = 3\n",
        "x = np.random.rand(n, c1)\n",
        "y = np.random.rand(n, c2)\n",
        "batches = get_random_batches(x, y, batch_size)\n",
        "assert type(batches) == list\n",
        "assert len(batches) >= 6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "deletable": false,
        "id": "WTNNt7ObdHTR",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "536e781d5a4a0684f44501d32a2be83d",
          "grade": false,
          "grade_id": "cell-96bdeec247659603",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def train(x: np.ndarray, y: np.ndarray, params: dict, batch_size: int = 5,\n",
        "          max_iters: int = 500, learning_rate: float=1e-3):\n",
        "    \n",
        "    \"\"\"\n",
        "    Train the network with two sequential layers: \n",
        "    (1) one layer named \"layer1\" with sigmoid activation\n",
        "    (2) one layer named \"output\" with softmax activation\n",
        "\n",
        "    [input]\n",
        "    * x -- training samples\n",
        "    * y -- training lables\n",
        "    * params -- a dictionary containing initial parameters\n",
        "    * batch_size -- batch size\n",
        "    * max_iters -- total number of iterations\n",
        "    * learning_rate -- learning rate\n",
        "    \n",
        "    [output]\n",
        "    * total_loss, avg_acc -- loss and accuracy for the last iteration\n",
        "    \"\"\"\n",
        "\n",
        "    batches = get_random_batches(x, y, batch_size)\n",
        "\n",
        "    for itr in range(max_iters):\n",
        "        total_loss = 0\n",
        "        avg_acc = 0\n",
        "        num_batches = len(batches)\n",
        "        for xb, yb in batches:\n",
        "\n",
        "            # forward\n",
        "            layer_1_out = forward(X = xb, params = params, name = \"layer1\", activation = sigmoid)\n",
        "            probs       = forward(X = layer_1_out, params = params, name = \"output\", activation = softmax)\n",
        "            \n",
        "            # loss\n",
        "            # be sure to add loss and accuracy to epoch totals\n",
        "            loss_b, acc_b = compute_loss_and_acc(y = yb, probs = probs)\n",
        "            total_loss += loss_b\n",
        "            avg_acc  += float (1 / num_batches) * float(acc_b)\n",
        "            \n",
        "            # backward\n",
        "            #delta_2 = backwards(delta = loss_b, params = params, name = \"output\", activation_deriv = linear_deriv)\n",
        "            # dJ/dWout = h_L * (yhat - y)\n",
        "            delta_2 = backwards(delta = probs - yb, params = params, name = \"output\", activation_deriv=linear_deriv)\n",
        "            grad    = backwards(delta = delta_2, params = params, name = \"layer1\", activation_deriv = sigmoid_deriv)\n",
        "\n",
        "            # apply gradient\n",
        "            # Gradient descent\n",
        "            params[\"Woutput\"] = params[\"Woutput\"] - learning_rate * params[\"grad_Woutput\"]\n",
        "            params[\"Wlayer1\"] = params[\"Wlayer1\"] - learning_rate * params[\"grad_Wlayer1\"]\n",
        "            params[\"boutput\"] = params[\"boutput\"] - learning_rate * params[\"grad_boutput\"]\n",
        "            params[\"blayer1\"] = params[\"blayer1\"] - learning_rate * params[\"grad_blayer1\"]\n",
        "        \n",
        "        if itr % 100 == 0:\n",
        "            print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(\n",
        "                itr, total_loss, avg_acc))\n",
        "\n",
        "    return total_loss, avg_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "bK7r6gBTdHTS",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b42a354df1f71bfacbf65947af28e7f0",
          "grade": true,
          "grade_id": "q1_5_1_train",
          "locked": true,
          "points": 8,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "0037851c-0ffe-47ab-c142-2f60fb927da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "itr: 00 \t loss: 58.23 \t acc : 0.00\n",
            "itr: 100 \t loss: 45.61 \t acc : 0.50\n",
            "itr: 200 \t loss: 38.63 \t acc : 0.60\n",
            "itr: 300 \t loss: 34.13 \t acc : 0.65\n",
            "itr: 400 \t loss: 31.33 \t acc : 0.65\n",
            "itr: 500 \t loss: 29.57 \t acc : 0.68\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\chris\\dev\\16720-ComputerVision\\hw3\\q1.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw3/q1.ipynb#X54sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mitr: \u001b[39m\u001b[39m{:02d}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m acc : \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m500\u001b[39m, total_loss, avg_acc))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw3/q1.ipynb#X54sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# with default settings, you should get loss < 35 and accuracy > 70%\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw3/q1.ipynb#X54sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39massert\u001b[39;00m total_loss \u001b[39m<\u001b[39m \u001b[39m35\u001b[39m \u001b[39mand\u001b[39;00m avg_acc \u001b[39m>\u001b[39m \u001b[39m0.70\u001b[39m\n",
            "\u001b[1;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Successulf implementation of dependent functions are required to get full score for the `train` function\n",
        "\n",
        "# create inputs\n",
        "g0 = np.random.multivariate_normal([3.6,40],[[0.05,0],[0,10]],10)\n",
        "g1 = np.random.multivariate_normal([3.9,10],[[0.01,0],[0,5]],10)\n",
        "g2 = np.random.multivariate_normal([3.4,30],[[0.25,0],[0,5]],10)\n",
        "g3 = np.random.multivariate_normal([2.0,10],[[0.5,0],[0,10]],10)\n",
        "x = np.vstack([g0,g1,g2,g3])\n",
        "\n",
        "# create labels\n",
        "y_idx = np.array([0 for _ in range(10)] + [1 for _ in range(10)] + [2 for _ in range(10)] + [3 for _ in range(10)])\n",
        "\n",
        "# turn labels to one_hot\n",
        "y = np.zeros((y_idx.shape[0],y_idx.max()+1))\n",
        "y[np.arange(y_idx.shape[0]),y_idx] = 1\n",
        "\n",
        "# parameters in a dictionary\n",
        "params = {}\n",
        "# initialize a layer\n",
        "initialize_weights(2,25,params,'layer1')\n",
        "initialize_weights(25,4,params,'output')\n",
        "\n",
        "# train the two-layer neural network\n",
        "total_loss, avg_acc = train(x, y, params, batch_size=5, max_iters=500, learning_rate=1e-3)\n",
        "print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(500, total_loss, avg_acc))\n",
        "\n",
        "# with default settings, you should get loss < 35 and accuracy > 70%\n",
        "assert total_loss < 35 and avg_acc > 0.70\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "nsGLyrfSdHTT",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d8899e5620b778603871026cee269bbc",
          "grade": false,
          "grade_id": "cell-912ef865c23093b7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Q1.6 Numerical Gradient Checker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ID_PfJwldHTT",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5580c4ba31d85d2e2c246aa0e64ed48d",
          "grade": false,
          "grade_id": "cell-da874ad741d95189",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Q1.6.1 (15 points, auto-grader)\n",
        "Implement the `centeral_differences_gradient` function. Instead of using the analytical gradients computed from the chain rule, add $\\epsilon$ offset to each element in the weights, and compute the numerical gradient of the loss with central differences. Central differences is just $\\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2 \\epsilon}$. Remember, this needs to be done for each scalar dimension in all of your weights independently. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def centeral_differences_gradient(params: dict, eps = 1e-6):\n",
        "    \"\"\"\n",
        "    Compute the estimated gradients using central difference\n",
        "    \n",
        "    Hint:\n",
        "    please feel free to reuse the functions above\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for k, v in params.items():\n",
        "        if '_' in k:\n",
        "            continue\n",
        "\n",
        "        if len(v.shape) == 2:\n",
        "          # Holds on to the gradient values for each weight/bias term\n",
        "          grad = np.zeros_like(v)\n",
        "\n",
        "          for idx in range(np.prod(v.shape)):\n",
        "            if len(grad.shape) == 2:\n",
        "              idx = (int(idx / grad.shape[1]), int(idx % grad.shape[1]))\n",
        "\n",
        "            # Left (- eps)\n",
        "            v[idx] -= eps\n",
        "            input_left = forward(x, params, \"layer1\", activation=sigmoid)\n",
        "            out_left = forward(input_left, params, \"output\", activation=softmax)\n",
        "            loss_left, _ = compute_loss_and_acc(y, out_left)\n",
        "\n",
        "            # Right (+ eps)\n",
        "            v[idx] += 2*eps\n",
        "            input_right = forward(x, params, \"layer1\", activation=sigmoid)\n",
        "            out_right = forward(input_right, params, \"output\", activation=softmax)\n",
        "            loss_right, _ = compute_loss_and_acc(y, out_right)\n",
        "\n",
        "            # Central Difference\n",
        "            grad[idx] = (loss_right - loss_left) / (2 * eps)\n",
        "\n",
        "            # Set it back to the old value\n",
        "            v[idx] -= eps\n",
        "\n",
        "          # Save the gradient\n",
        "          params[\"grad_\"+k] = grad\n",
        "\n",
        "    # Save to the params\n",
        "    # for key in out.keys():\n",
        "    #   print(key)\n",
        "    #   params[key] = out[key]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "deletable": false,
        "editable": false,
        "id": "N2oCyZpmdHTU",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7f38f50df99b3b1d8c1d3074a5a2b021",
          "grade": true,
          "grade_id": "q1_6_1",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "d57b8a98-c634-4613-a5d6-424ee81fedac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grad_Woutput Error: 1.9659158481792783e-06\n",
            "grad_boutput Error: 0.0\n",
            "grad_Wlayer1 Error: 1.2296770548214458e-06\n",
            "grad_blayer1 Error: 0.0\n",
            "Total Error: 3.195592903000724e-06\n",
            "Test case passed, good job!\n"
          ]
        }
      ],
      "source": [
        "# Compute the analytical gradients\n",
        "h1 = forward(x,params,'layer1')\n",
        "probs = forward(h1,params,'output',softmax)\n",
        "delta1 = probs\n",
        "delta1[np.arange(probs.shape[0]),y_idx] -= 1\n",
        "\n",
        "delta2 = backwards(delta1,params,'output',linear_deriv)\n",
        "backwards(delta2,params,'layer1',sigmoid_deriv)\n",
        "\n",
        "import copy\n",
        "params_orig = copy.deepcopy(params)\n",
        "\n",
        "# Compute the estimated gradient using central difference\n",
        "centeral_differences_gradient(params)\n",
        "\n",
        "total_error = 0\n",
        "for k in params.keys():\n",
        "    if 'grad_' in k:\n",
        "        # relative error\n",
        "        err = np.abs(params[k] - params_orig[k])/np.maximum(np.abs(params[k]),np.abs(params_orig[k]))\n",
        "        err = err.sum()\n",
        "        total_error += err\n",
        "        print(f\"{k} Error: {err}\")\n",
        "print(f\"Total Error: {total_error}\")\n",
        "# should be less than 1e-4\n",
        "assert 0. < total_error < 1e-4\n",
        "print('Test case passed, good job!')"
      ]
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('vision_conda')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ab1be24f2b69fea20ab96b72f6a75a8226e3980324f891cd62f88ac8e8b7a219"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
