{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a80ee45a7dde763ed37979dfddb89bea",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "## Neural Networks for Recognition - Assignment 3\n",
    "    Instructor: Kris                          TAs: Arka, Jinkun, Rawal, Rohan, Sheng-Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d23ab52a5796705fd330208f55a1aca",
     "grade": false,
     "grade_id": "cell-ee45598a54db40ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Theory Questions (45 points)\n",
    "**Grading**:  \n",
    "- The theory part consists of 7 questions.\n",
    "- Please add your answers to the writeup (submitted as pdf to HW3:PDF). Insert images whenever necessary.\n",
    "- Show all your work to obtain full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "331415a2eb1c44f27d792e9bfe06b47a",
     "grade": false,
     "grade_id": "theory_q2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1 (3 points)\n",
    "\n",
    "The softmax function can be defined as, $softmax(x_i)= \\frac{1}{S} s_i$ where $s_i = e^{x_i}$ , $S=\\sum_i s_i$. Using this definition, please answer Q1.1, Q1.2 and Q1.3 below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d5739e851b6472a72a8dab400606ed0",
     "grade": false,
     "grade_id": "cell-82ce863ef815b3b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.1 (1 point)\n",
    "Let $x \\in \\mathbb{R}^d$, what are the properties of $softmax(x)$, specifically, what is the range of each element in $softmax(x)$? What is the sum of all elements in $softmax(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61aacdeda627affd5fc5845996f1d72d",
     "grade": true,
     "grade_id": "theory_q2_ans",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e601d9a474cd8238bfd928279e38c4d",
     "grade": false,
     "grade_id": "cell-11b4b5748e0010bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2 (1 point)\n",
    "”Softmax takes an arbitrary real valued vector $x$ and turns it into a ___”. **Please fill in the blank using an appropriate word/phrase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed91437533a9e58883df69f77ea17eac",
     "grade": true,
     "grade_id": "cell-433a8c3d311f4d7a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "705218fbf3e7d15fbc223f44a096b08a",
     "grade": false,
     "grade_id": "cell-db431f37e7386bd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.3 (1 point)\n",
    "Let $x \\in \\mathbb{R}^d$, assume $v = softmax(softmax(... softmax(x)))$ where the softmax function is applied to $x$ recursively $N$ times. What is the value of $v$ as a function of $d$ $\\forall x \\in \\mathbb{R}^d$, in the limit $N \\rightarrow \\infty$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2aaabd020a614000076e273fff6e2bd",
     "grade": true,
     "grade_id": "cell-a52d68820bc5f158",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b13f448e7c97c97f5029349d1b16ce0",
     "grade": false,
     "grade_id": "theory_q1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2 (4 points)\n",
    "Prove that softmax is invariant to translation, that is \n",
    "$$softmax(x) = softmax(x + c) \\qquad \\forall c \\in \\mathbb{R}$$\n",
    "Again, softmax is defined as, for each index $i$ in a vector $x$.\n",
    "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\n",
    "Often we use $c = − \\max x_i$. Why is that a good idea? (Tip: consider the range of values that numerator will have with $c = 0$ and $c = − \\max x_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd84960af9fd395dabf70fa038e25a73",
     "grade": true,
     "grade_id": "theory_q1_ans",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba93aa4dd47896315d502eafc29b2c6e",
     "grade": false,
     "grade_id": "cell-38104f92a19a284a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3 (3 points)\n",
    "Show that the functions represented by a multi-layer fully-connected neural networks without a non-linear activation function are linear functions of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30381b92827e58e05fae85190eb15704",
     "grade": true,
     "grade_id": "cell-48454d1281c0524e",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceef2df72991ea5b48aeb0ac12155489",
     "grade": false,
     "grade_id": "cell-23409e49f2f8eb65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4 (4 points) \n",
    "Given the sigmoid activation function $\\sigma(x) = \\frac{1}{1+e^{-x}}$ , derive the gradient of the sigmoid function and show that it can be written solely as a function of $\\sigma(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f8509d97dc25f73bf21900d2e332113",
     "grade": true,
     "grade_id": "cell-c0ee4e1945d7414a",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab63520ddff815e9419408b34d681af4",
     "grade": false,
     "grade_id": "cell-b4ba2548034e7814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5 (12 points WriteUp)\n",
    "\n",
    "Given $y = W x + b$ (or $y_j = \\sum_{i=1}^d  x_{i} W_{ji} + b_j$), and the gradient of some loss $J$ with respect $y$, show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$. Be sure to do the derivatives with scalars and re-form the matrix form afterwards. Here are some helpful notations.\n",
    "$$ \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{k \\times 1} \\quad W \\in \\mathbb{R}^{k \\times d} \\quad x \\in \\mathbb{R}^{d \\times 1} \\quad b \\in \\mathbb{R}^{k \\times 1}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7489c88f7705b67573e66e63b4d066e6",
     "grade": true,
     "grade_id": "cell-d532eedba2ec9905",
     "locked": false,
     "points": 12,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05edab7c7dad1246cddc3d17936b7b93",
     "grade": false,
     "grade_id": "cell-efa5d9ebad394fed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6 (15 points)\n",
    "\n",
    "We will find the derivatives for Conv layers now. Since most Deep Learning frameworks such as Pytorch, Tensorflow use cross-correlation in their respective \"convolution\" functions ([Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) and [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/convolution)), we will continue this abuse of notation. So the operation performed with the Conv Layer weights will be cross-correlation.\n",
    "    \n",
    "The input, $x$ is of shape $M\\times N$ with C channels. This will be *convolved* (actually cross-correlation) with $D$ number of $K\\times K$ filters, each with a bias term. The stride is 1 and there will be no padding. We know the gradient of some loss $J$ with respect to the output $y$, which will have $D$ channels. Show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$.\n",
    "\n",
    "The dimensions and notation are as follows:\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{D\\times M_o \\times N_o}\n",
    "    \\quad\n",
    "    M_o = M-K+1\n",
    "    \\quad\n",
    "    N_o = N-K+1\n",
    "$$\n",
    "$$\n",
    "    x \\in \\mathbb{R}^{C\\times M \\times N}\n",
    "    \\quad\n",
    "    W \\in \\mathbb{R}^{D\\times C \\times K \\times K}\n",
    "    \\quad\n",
    "    b \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "$x_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the input\n",
    "\n",
    "$y_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the output\n",
    "\n",
    "$W_{d, c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column, the $c^{th}$ channel of the kernel of the $d^{th}$ filter\n",
    "\n",
    "*For this question, you may compute the derivatives with scalars only. You don't need to re-form the matrix*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3624c404829a26b2682c1f41740dc2af",
     "grade": true,
     "grade_id": "cell-b02303c52179085f",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eeeba4956195062fd9e58077f0fa9f8a",
     "grade": false,
     "grade_id": "cell-775c17b4eb8e7758",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q7 (4 points)\n",
    "\n",
    "When the neural network applies the elementwise activation function (such as sigmoid), the gradient of the activation function scales the back-propagation update. This is directly from the chain rule, $\\frac{d}{d x} f(g(x)) = f'(g(x)) g'(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b380b2b0ebb74f689d00c30fab3fdf26",
     "grade": false,
     "grade_id": "cell-4c0ef8ab732dda1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1 (1 point)\n",
    "Consider the sigmoid activation function for deep neural networks. Why might it lead to a \"vanishing gradient\" problem if it is used for many layers (consider plotting the $\\sigma'(x)$ in Q1.4)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "160f0d9b648792cd8f4b786a13d15578",
     "grade": true,
     "grade_id": "cell-cecc47088410bfc3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99bef598365d14fdd07e298ac59e55c5",
     "grade": false,
     "grade_id": "cell-c01f226eb7134484",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.2 (1 point)\n",
    "Often it is replaced with $\\tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}}$. What are the output ranges of both $\\tanh$ and sigmoid? Why might we prefer $\\tanh$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6f866c6c71afe439ec39b1b4c5b8012",
     "grade": true,
     "grade_id": "cell-359e85eddc00f567",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "889dabb3ca92c8a774f432172f58867e",
     "grade": false,
     "grade_id": "cell-adfb2179efada3bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.3 (1 point)\n",
    "Why does $\\tanh(x)$ have less of a vanishing gradient problem? (plotting the derivatives helps! for reference: $\\tanh'(x) = 1 - \\tanh(x)^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5832ee3b18a8a57b28840e7ab6cb7f9e",
     "grade": true,
     "grade_id": "cell-c9f65d287cf03f87",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55afcde21a8360b35531aea4b919dd84",
     "grade": false,
     "grade_id": "cell-a1326ba9e015ed2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.4 (1 point)\n",
    "$\\tanh$ is a scaled and shifted version of the sigmoid. Show how $\\tanh(x)$ can be written in terms of $\\sigma(x)$. (*Hint: consider how to make it have the same range*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "965e794e6e31a8d771c8653489b4d1d6",
     "grade": true,
     "grade_id": "cell-1608d4ea95217d89",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "8a14d3cb3a8a01d6659ca21286c75f37ed68cd344cf818a36ad292d43e0de27a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
