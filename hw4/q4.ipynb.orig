{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad363c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 2,
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
   "id": "champion-insertion",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61667d6bbd6be96ad5857a5e71661ad1",
     "grade": false,
     "grade_id": "q4-code1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_20120\\2381338451.py:8: DeprecationWarning: Please use `distance_transform_edt` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  from scipy.ndimage.morphology import distance_transform_edt\n"
     ]
    }
   ],
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
   "source": [
    "# DO NOT MODIFY\n",
    "import nbimporter\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage.morphology import distance_transform_edt\n",
    "\n",
    "from q2 import briefLite,briefMatch,plotMatches\n",
    "from q3 import computeH_ransac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-trauma",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98071ca7789df027a231fc2a1fc3b6fc",
     "grade": false,
     "grade_id": "q4-note1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 4: Stitching Panoramas (15 points)\n",
    "\n",
    "We can also use homographies to create a panorama image from multiple views of the same scene. This is possible for example when there is no camera translation between the views (e.g., only rotation about the camera center). First, you will generate panoramas using matched point correspondences between images using the BRIEF matching. **We will assume that there is no error in your matched point correspondences between images (Although there might be some errors, and even small errors can have drastic impacts)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-worth",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47548865813abe74bbe88eb877170ce4",
     "grade": false,
     "grade_id": "q4-note2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.1 Image Stitching (5 pts)\n",
    "\n",
    "In this problem you will implement and use the function: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\texttt{[panoImg] = imageStitching(img1, img2, H2to1)}\n",
    "\\end{equation}\n",
    "\n",
    "on two images from the Dusquesne incline. This function accepts two images and the output from the homography estimation function. This function:\n",
    "\n",
    "   1. Warps $\\texttt{img2}$ into $\\texttt{img1}'s$ reference frame using the OpenCV $\\texttt{warpPerspective}$ function;\n",
    "   2. Blends $\\texttt{img1}$ and warped $\\texttt{img2}$ and outputs the panorama image. \n",
    "\n",
    "For this problem, use the provided images $\\texttt{pnc1}$ as $\\texttt{img1}$ and $\\texttt{pnc0}$ as $\\texttt{img2}$. The point correspondences $\\texttt{pts}$ are generated by your BRIEF descriptor matching.\n",
    "\n",
    "Apply your $\\texttt{computeH_ransac()}$ to these correspondences to compute $\\texttt{H2to1}$, which is the homography from $\\texttt{pnc0}$ onto $\\texttt{pnc1}$. Then apply this homography to $\\texttt{pnc0}$ using $\\texttt{cv2.warpPerspective()}$.\n",
    "\n",
    "**This question will be manually graded. Visualize the warped image. Please include the image and your H2to1 matrix (with the bottom right index as 1) in your writeup PDF, along with stating which image pair you used.**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 3,
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
   "id": "working-thesis",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1457b2241342b6cac2ca8d40ea281f58",
     "grade": false,
     "grade_id": "q4-code2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.48757218e-01 -3.48406319e-02  4.07883151e+02]\n",
      " [-7.67792390e-02  8.88049118e-01 -1.95702169e+01]\n",
      " [-3.47945083e-04  2.11881269e-06  1.00000000e+00]]\n"
     ]
    }
   ],
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
   "source": [
    "def imageStitching(im1, im2, H2to1):\n",
    "    '''\n",
    "    Returns a panorama of im1 and im2 using the given \n",
    "    homography matrix\n",
    "\n",
    "    INPUT\n",
    "        Warps img2 into img1 reference frame using the provided warpH() function\n",
    "        H2to1 - a 3 x 3 matrix encoding the homography that best matches the linear\n",
    "                 equation.\n",
    "    OUTPUT\n",
    "        img_pano - the panorama image.\n",
    "    '''\n",
<<<<<<< HEAD
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return img_pano"
=======
    "    \n",
    "    # Warp the second image (right) to the reference image 1 (left)\n",
    "    # Literally just warp 2 to 1 then slap 1 on top of it\n",
    "    img_pano = cv2.warpPerspective(im2, H2to1, (im1.shape[1]+im2.shape[1], im1.shape[0]))\n",
    "    img_pano[0:im1.shape[0], 0:im1.shape[1]] = im1\n",
    "\n",
    "    return img_pano\n",
    "\n",
    "# # Load the incline images\n",
    "im1 = cv2.imread(\"data/incline_L.png\")\n",
    "im2 = cv2.imread(\"data/incline_R.png\")\n",
    "im1 = cv2.resize(im1, (im2.shape[1], im2.shape[0]))\n",
    "\n",
    "# Find the locations of the feature matches\n",
    "locs1, desc1 = briefLite(im1)\n",
    "locs2, desc2 = briefLite(im2)\n",
    "matches = briefMatch(desc1, desc2)\n",
    "\n",
    "np_matches = np.array(matches)\n",
    "\n",
    "H, inliers = computeH_ransac(np_matches, locs1, locs2)\n",
    "print(H)\n",
    "\n",
    "# Stitch the images together\n",
    "pano_img = imageStitching(im1, im2, H)\n",
    "cv2.imshow(\"Panorama\", pano_img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-olympus",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7407c460d720c6dee25dd559fe60b86",
     "grade": false,
     "grade_id": "q4-note3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2 Image Stitching with No Clip (3 pts)\n",
    "\n",
    "Notice how the output from Q 4.1 is clipped at the edges? We will fix this now. Implement a function \n",
    "\n",
    "\\begin{equation}\n",
    "    \\texttt{[panoImg] = imageStitching_noClip(img1, img2, H2to1)}\n",
    "\\end{equation}\n",
    "\n",
    "that takes in the same input types and produces the same outputs as in Q 4.1.\n",
    "\n",
    "To prevent clipping at the edges, we instead need to warp _both_ image 1 and image 2 into a common third reference frame in which we can display both images without any clipping. Specifically, we want to find a matrix $M$ that _only_ does scaling and translation such that:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\texttt{warp_im1 = CV2.warpPerspective(im1, M, out_size);}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\texttt{warp_im2 = CV2.warpPerspective(im2, M*H2to1, out_size);}\n",
    "\\end{equation}\n",
    "\n",
    "This produces warped images in a common reference frame where all points in $\\texttt{im1}$ and $\\texttt{im2}$ are visible. To do this, we will only take as input either the width or height of $\\texttt{out_size}$ and compute the other one based on the given images such that the warped images are not squeezed or elongated in the panorama image. For now, assume we only take as input the width of the image (i.e., $\\texttt{out_size(2)}$) and should therefore compute the correct height(i.e., $\\texttt{out_size(1)}$).\n",
    "\n",
    "_Hint:_ The computation will be done in terms of $\\texttt{H2to1}$ and the extreme points (corners) of the two images.\n",
    "Make sure $M$ includes only scale (find the aspect ratio of the full-sized panorama image) and translation. \n",
    "\n",
    "**This question will be manually graded. Visualize the warped image. Please include the image in your writeup PDF, along with stating which image pair you used.**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 8,
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
   "id": "annual-paraguay",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a90ab79822bccce01191736b2187597",
     "grade": false,
     "grade_id": "q4-code3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def imageStitching_noClip(im1, im2, H2to1):\n",
=======
    "def imageStitching_noClip(im1, im2, H2to1, out_width = 1080):\n",
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
    "    '''\n",
    "    Returns a panorama of im1 and im2 using the given \n",
    "    homography matrix without cliping.\n",
    "    \n",
    "    INPUTS\n",
    "        im1 and im2 - images to be stitched.\n",
    "        H2to1- the homography matrix.\n",
    "    OUTPUT\n",
    "        img_pano - the panorama image.\n",
    "    ''' \n",
<<<<<<< HEAD
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return img_pano"
=======
    "    # Get the corners of the two images\n",
    "    corners_1 = np.array([[0, 0, im1.shape[1], im1.shape[1]-1], [0, im1.shape[0]-1, 0, im1.shape[1]-1], [1, 1, 1, 1]])\n",
    "    corners_2 = np.array([[0, 0, im2.shape[1], im2.shape[1]-1], [0, im2.shape[0]-1, 0, im2.shape[0]-1], [1, 1, 1, 1]])\n",
    "    \n",
    "    # Warp the corners\n",
    "    corners_2_frame1 = H2to1 @ corners_2\n",
    "    corners_2_frame1 /= corners_2_frame1[-1] # Convert the homogenous coordinates\n",
    "\n",
    "    # Look at the output width and height\n",
    "    min_H = min(0, np.min(corners_2_frame1[1]))\n",
    "    max_H = max(im1.shape[0], np.max(corners_2_frame1[1]))\n",
    "    min_W = min(0, np.min(corners_2_frame1[0]))\n",
    "    max_W = max(im1.shape[1], np.max(corners_2_frame1[0]))\n",
    "\n",
    "    # The size needed to properly fit both images\n",
    "    calc_H = max_H - min_H\n",
    "    calc_W = max_W - min_W\n",
    "    out_height = int(out_width * calc_H/calc_W)\n",
    "    out_size = (out_width, out_height)\n",
    "\n",
    "    # Get the amount of scaling needed to get image 1 to the new aspect ratio\n",
    "    scale = out_width / (max_W - min_W)\n",
    "\n",
    "    M = np.array([[scale,0, 0], [0, scale, 0], [0,0,1]])\n",
    "    # M = np.array([[1,0,0], [0, 1, -min_H], [0,0,1]])\n",
    "\n",
    "    warp_im1 = cv2.warpPerspective(im1, M, out_size)\n",
    "    warp_im2 = cv2.warpPerspective(im2, M@H2to1, out_size)\n",
    "    img_pano = np.maximum(warp_im1, warp_im2)\n",
    " \n",
    "    # cv2.imshow(\"Im1\", warp_im1)\n",
    "    # cv2.waitKey()\n",
    "    # cv2.imshow(\"Im2\", warp_im2)\n",
    "    # cv2.waitKey()\n",
    "    # cv2.destroyAllWindows()\n",
    "    return img_pano\n",
    "\n",
    "\n",
    "# # Load the incline images\n",
    "# im1 = cv2.imread(\"data/incline_L.png\")\n",
    "# im2 = cv2.imread(\"data/incline_R.png\")\n",
    "# im1 = cv2.resize(im1, (im2.shape[1], im2.shape[0]))\n",
    "\n",
    "# # Find the locations of the feature matches\n",
    "# locs1, desc1 = briefLite(im1)\n",
    "# locs2, desc2 = briefLite(im2)\n",
    "# matches = briefMatch(desc1, desc2)\n",
    "\n",
    "# np_matches = np.array(matches)\n",
    "\n",
    "# H, inliers = computeH_ransac(np_matches, locs1, locs2)\n",
    "# print(H)\n",
    "\n",
    "# # Stitch the images together\n",
    "# pano_img = imageStitching_noClip(im1, im2, H)\n",
    "# cv2.imshow(\"Panorama\", pano_img)\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()"
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-provision",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d87eb810c0d391c86053ae5585358be7",
     "grade": false,
     "grade_id": "q4-note4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.3 Generate Panorama (2 pts)\n",
    "\n",
    "You now have all the tools you need to automatically generate panoramas. Write a function that accepts two images as input, computes keypoints and descriptors for both the images, finds putative feature correspondences by matching keypoint descriptors, estimates a homography using RANSAC and then warps one of the images with the homography so that they are aligned and then overlays them. \n",
    "\n",
    "\\begin{equation}\n",
    "\\texttt{im3 = generatePanorama(im1, im2)}\n",
    "\\end{equation}\n",
    "\n",
    "Run your code on the image pair $\\texttt{data/pnc1.jpg}$, $\\texttt{data/pnc0.jpg}$ or $\\texttt{data/incline_L.jpg}$, $\\texttt{data/incline_R.jpg}$. However during debugging, try on scaled down versions of the images to keep running time low. \n",
    "\n",
    "**This question will be manually graded. Save the resulting panorama on the full sized images and include the figure and computed homography matrix in your writeup.**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 6,
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
   "id": "incomplete-election",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad016a168b1c7ce13a3d1a5499da00b",
     "grade": false,
     "grade_id": "q4-code4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generatePanorama(im1, im2):\n",
    "    '''\n",
    "    Gnerate a panorama from two images.\n",
    "    \n",
    "    INPUTS\n",
    "        im1 and im2 - images to be stitched.\n",
    "    OUTPUT\n",
    "        img_pano - the panorama image.\n",
    "    '''\n",
<<<<<<< HEAD
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return img_pano"
=======
    "    # Find the locations of the feature matches\n",
    "    locs1, desc1 = briefLite(im1)\n",
    "    locs2, desc2 = briefLite(im2)\n",
    "    matches = briefMatch(desc1, desc2)\n",
    "\n",
    "    np_matches = np.array(matches)\n",
    "\n",
    "    H, inliers = computeH_ransac(np_matches, locs1, locs2)\n",
    "    print(H)\n",
    "\n",
    "    # Stitch the images together\n",
    "    pano_img = imageStitching_noClip(im1, im2, H)\n",
    "    return pano_img\n",
    "\n",
    "    "
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-material",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12dffddbf14cf22ddb2d323735ade9f3",
     "grade": false,
     "grade_id": "q4-note5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.4 extra credits (3 pts)\n",
    "\n",
    "Collect a pair of your own images (with your phone) and stitch them together using your code from the previous section. Include the pair of images and their result in the write-up."
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": 7,
   "id": "a63367f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.23947148e+00  3.12029051e-02 -3.64763210e+02]\n",
      " [ 6.41093210e-02  1.17422210e+00 -7.03326788e+01]\n",
      " [ 2.07849973e-04  5.04027896e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "img1 = cv2.imread(\"data/1.jpg\")\n",
    "img2 = cv2.imread(\"data/2.jpg\")\n",
    "img1 = cv2.resize(img1, (1024, 720))\n",
    "img2 = cv2.resize(img2, (1024, 720))\n",
    "pano_img = generatePanorama(img2, img1)\n",
    "cv2.imshow(\"Panorama\", pano_img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
   "cell_type": "markdown",
   "id": "gentle-gibraltar",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa73ce8272fcbe4bb3c0bcadc1cae4ef",
     "grade": false,
     "grade_id": "q4-note6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.5 extra credits (2 pts)\n",
    "\n",
    "Collect at least 6 images  and stitch them into a single noClip image. You can either collect your own, or use the [PNC Park images](http://www.cs.jhu.edu/~misha/Code/SMG/PNC3.zip) from Matt Uyttendaele. We used the PNC park images (subsmapled to 1/4 sized) and ORB keypoints and descriptors for our reference solution."
   ]
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9465e140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.77233628e-01 -1.37203270e-02  2.88513449e+02]\n",
      " [-8.36630806e-02  9.26471396e-01  4.36095754e+01]\n",
      " [-2.17794136e-04 -6.88387579e-06  1.00000000e+00]]\n",
      "[[ 1.68036879e-01 -6.95314016e-01  3.68345175e+02]\n",
      " [ 4.39731583e-02 -4.47150110e-01  2.69146132e+02]\n",
      " [ 4.09493857e-04 -1.88076840e-03  1.00000000e+00]]\n",
      "[[ 9.46795581e-01  4.13954526e-02  2.82158652e+02]\n",
      " [ 2.22034566e-02  9.89649298e-01 -4.15436122e+00]\n",
      " [-6.61509372e-05  4.32614024e-06  1.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\dev\\16720-ComputerVision\\hw4\\keypointDetect.py:87: RuntimeWarning: divide by zero encountered in divide\n",
      "  principal_curvature = np.divide(np.square(np.add(gxx, gyy)), (np.multiply(gxx, gyy) - np.multiply(gxy, gyx)))\n",
      "c:\\Users\\chris\\dev\\16720-ComputerVision\\hw4\\keypointDetect.py:87: RuntimeWarning: invalid value encountered in divide\n",
      "  principal_curvature = np.divide(np.square(np.add(gxx, gyy)), (np.multiply(gxx, gyy) - np.multiply(gxy, gyx)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.19057755e-01  1.56480834e+00 -8.38486127e+01]\n",
      " [-6.61254383e+00  1.66583504e-01  9.65877037e+02]\n",
      " [-7.41855277e-03  6.38420990e-04  1.00000000e+00]]\n",
      "[[ 2.09911549e-01 -5.05695582e-01  4.36564086e+02]\n",
      " [-1.10511390e-01 -4.92291519e-01  5.94808084e+02]\n",
      " [-2.13491225e-05 -9.09470888e-04  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "img1 = cv2.imread(\"data/1.jpg\")\n",
    "img2 = cv2.imread(\"data/2.jpg\")\n",
    "img3 = cv2.imread(\"data/3.jpg\")\n",
    "img4 = cv2.imread(\"data/4.jpg\")\n",
    "img5 = cv2.imread(\"data/5.jpg\")\n",
    "img6 = cv2.imread(\"data/6.jpg\")\n",
    "\n",
    "img1 = cv2.resize(img1, (img1.shape[0]//4, img1.shape[1]//4))\n",
    "img2 = cv2.resize(img2, (img2.shape[0]//4, img2.shape[1]//4))\n",
    "img3 = cv2.resize(img3, (img3.shape[0]//4, img3.shape[1]//4))\n",
    "img4 = cv2.resize(img4, (img4.shape[0]//4, img4.shape[1]//4))\n",
    "img5 = cv2.resize(img5, (img5.shape[0]//4, img5.shape[1]//4))\n",
    "img6 = cv2.resize(img6, (img6.shape[0]//4, img6.shape[1]//4))\n",
    "\n",
    "pano_img_1 = generatePanorama(img1, img2)\n",
    "pano_img_2 = generatePanorama(img3, img4)\n",
    "pano_img_3 = generatePanorama(img5, img6)\n",
    "\n",
    "pano_img_1 = cv2.resize(pano_img_1, (1080, 1280))\n",
    "pano_img_2 = cv2.resize(pano_img_2, (1080, 1280))\n",
    "pano_img_3 = cv2.resize(pano_img_3, (1080, 1280))\n",
    "\n",
    "pano_comb_1 = generatePanorama(pano_img_1, pano_img_2)\n",
    "cv2.imshow(\"Panorama2\", pano_comb_1)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.resize(pano_comb_1, (1080, 1280))\n",
    "final       = generatePanorama(pano_comb_1, pano_img_3)\n",
    "cv2.imshow(\"Panorama\", final)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
=======
   "display_name": "Python 3.9.13 ('vision_conda')",
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.8"
=======
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab1be24f2b69fea20ab96b72f6a75a8226e3980324f891cd62f88ac8e8b7a219"
   }
>>>>>>> b340a2d4da61648edb00a3ce28ab68f64254d13e
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
