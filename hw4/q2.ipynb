{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee0ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-tenant",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e058e539b0238fec3688fe5c754729f",
     "grade": false,
     "grade_id": "q2-code0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6cbae1-7474-45fa-a9f5-94d0a7497074",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05176a5b4d550bb6b67f9024f9971f94",
     "grade": false,
     "grade_id": "q2-code1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from scipy.spatial.distance import cdist\n",
    "from keypointDetect import DoGdetector\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41887f43-518c-4bf0-8a3b-e212067744f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66dca41fb3fef9711938dff56c1c1023",
     "grade": false,
     "grade_id": "q2-note1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Interest Points\n",
    "\n",
    "Before finding the homography between an image pair, we need to find corresponding point pairs between two images. But how do we get these points? One way is to select them manually (using `cpselect`), which is tedious and inefficient. The CV way is to find interest points in the image pair and automatically match them. In the interest of being able to do cool stuff, we will not re-implement a feature detector here, but use built-in methods. The purpose of an interest point detector (e.g. Harris, FAST, SIFT, SURF, etc.) is to find particular salient points in the images around which we extract feature descriptors (e.g. BRIEF, ORB, etc.). These descriptors try to summarize the content of the image around the feature points in as succinct yet descriptive manner possible (there is often a trade-off between representational and computational complexity for many computer vision tasks; you can have a very high dimensional feature descriptor that would ensure that you get good matches, but computing it could be prohibitively expensive). Matching, then, is a task of trying to find a descriptor in the list of descriptors obtained after computing them on a new image that best matches the current descriptor. This could be something as simple as the Euclidean distance between the two descriptors, or something more complicated, depending on how the descriptor is composed. \n",
    "\n",
    "For the purpose of this exercise, you can use FAST or Harris detector in concert with the BRIEF descriptor. You can use Python functions [skimage.feature](http://scikit-image.org/docs/0.13.x/api/skimage.feature.html) such as [skimage.feature.corner_harris](http://scikit-image.org/docs/0.13.x/api/skimage.feature.htm#skimage.feature.corner_harris) or [skimage.feature.corner_fast](http://scikit-image.org/docs/0.13.x/api/skimage.feature.html#skimage.feature.corner_fast).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30050095-971d-4d73-aaef-f8d479c87402",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b82ff9bc5f5325dca646ba9777f56817",
     "grade": false,
     "grade_id": "q2-note2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2: BRIEF Descriptor (25 points)\n",
    "\n",
    "Now that we have interest points that tell us where to find the most informative\n",
    "points in the image, we can compute descriptors that can be used to match to\n",
    "other views of the same point in different images. The BRIEF descriptor encodes\n",
    "information from a $9\\times9$ patch $p$ centered around the interest point at\n",
    "the _characteristic scale_ of the interest point. See the lecture notes for\n",
    "Point Feature Detectors if you need to refresh your memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645781c3-2432-4b15-8b3c-9496a01eaf8c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ed7c28cd6e908702d5690ffa1f8149b",
     "grade": false,
     "grade_id": "q2-note3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Creating a Set of BRIEF Tests (5 pts)\n",
    "\n",
    "The descriptor itself is a vector that is $n$-bits long, where each bit is the\n",
    "result of the following simple test:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tau(p;x,y):=\\begin{cases}\n",
    "    1, & \\text{if $p(x)<p(y)$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "Set $n$ to 256 bits. There is no need to encode the test results as actual bits. It is fine to encode them as a 256 element vector.\n",
    "\n",
    "There are many choices for the 256 test pairs $(x,y)$ used to compute\n",
    "$\\tau(p;x,y)$ (each of the $n$ bits). The authors describe and test\n",
    "some of them in \\[**Calonder**\\]. Read section 3.2 of that paper and implement\n",
    "one of these solutions. You should generate a static set of test pairs and save\n",
    "that data to a file. You will use these pairs for all subsequent computations of\n",
    "the BRIEF descriptor. \n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d36dcc-d980-4ed1-a2bf-4292b154d3fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcdb44df43fc09e306ea295a1feb05b9",
     "grade": false,
     "grade_id": "q2-note4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q 2.1\n",
    "\n",
    "Write the function to create the $x$ and $y$\n",
    "pairs that we will use for comparison to compute $\\tau$:\n",
    "\n",
    "\\begin{equation}\n",
    "    {\\tt [compareX, compareY] = makeTestPattern(patchWidth, nbits) }\n",
    "\\end{equation}\n",
    "\n",
    "${\\tt patchWidth }$ is the width of the image patch (usually $9$) and ${\\tt nbits }$\n",
    "is the number of tests $n$ in the BRIEF descriptor. ${\\tt compareX }$ and\n",
    "${\\tt compareY }$ are are each $nbits\\times 1$ vectors linear indices, and the indice values are in $[0, {\\texttt{patchWidth} \\times \\texttt{patchWidth}})$. Run this\n",
    "routine for the given parameters $\\texttt{patchWidth}=9$ and $n=256$ and save\n",
    "the results in ${\\tt testPattern.npy }$. **Include this file** in your submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-confirmation",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6f71dc7f41faefc19373461ac1b46d0",
     "grade": false,
     "grade_id": "q2_1_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def makeTestPattern(patch_width=9, nbits=256):\n",
    "    '''\n",
    "    Creates Test Pattern for BRIEF.\n",
    "    Run this routine for the given parameters patch_width = 9 and n = 256.\n",
    "\n",
    "    INPUTS\n",
    "    patch_width - the width of the image patch (usually 9)\n",
    "    nbits      - the number of tests n in the BRIEF descriptor\n",
    "\n",
    "    OUTPUTS\n",
    "    compareX and compareY - LINEAR indices into the patch_width x patch_width image \n",
    "                            patch and are each (nbits,) vectors. \n",
    "    '''\n",
    "    # Hint: Technically, you can use any patterns you want, as long as all the images\n",
    "    # use the same pattern.  Random indices could be a solution.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    np.save('data/testPattern.npy', [compareX, compareY])\n",
    "    return  compareX, compareY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-works",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c10ba803206ae27f126d044927c8e64",
     "grade": true,
     "grade_id": "q2_1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### HIDDEN TEST CELL\n",
    "# Hint: We will check the existance of the file and the shape of the\n",
    "# indice vectors. Do not modify the following parameters\n",
    "compareX, compareY = makeTestPattern(patch_width=9, nbits=256)\n",
    "# change the following path to where you save the file\n",
    "test_pattern_file = 'data/testPattern.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-swedish",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0efb1b69fac746cb3e904deb92ec6d2",
     "grade": false,
     "grade_id": "q2-note5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Compute the BRIEF Descriptor (5 pts) \n",
    "\n",
    "#### Q 2.2 \n",
    "\n",
    "It is now time to compute the BRIEF descriptor for the detected keypoints.  We need a function similar to the following one to do this:\n",
    "\n",
    "\\begin{equation}\n",
    "{\\tt [locs,desc] = computeBrief(im, locs, compareX, compareY)}\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\texttt{im}$ is a grayscale image with values from 0 to 1, $\\texttt{locs}$ are the keypoint\n",
    "locations returned by a keypoint detector. ${\\tt\n",
    "compareX}$ and ${\\tt compareY}$ are the test patterns computed in\n",
    "Section 2.1 and were saved into ${\\tt testPattern.npy}$.\n",
    "\n",
    "In this problem, you are going to use the provided $\\texttt{DoGdetector}$ function to detect keypoints. This function returns two outputs: The first one is ${\\tt locsDoG}$, $(m, 3)$, where the first two columns are the image coordinates of keypoints and the last column is the Gaussian pyramid levels from which the keypoints were detected. The second output is ${\\tt gaussPyramid}$, $(h, w, l)$, where $(h, w)$ is the image dimension and $l$ is the number of levels. Instead of using $\\texttt{im}$ as the input of ${\\tt computeBrief()}$, we are going to use ${\\tt gaussPyramid}$ in this problem:\n",
    "\n",
    "\\begin{equation}\n",
    "{\\tt [locs,desc] = computeBrief(gaussPyramid, locsDoG, compareX, compareY)}\n",
    "\\end{equation}\n",
    "\n",
    "For computing the BRIEF of a keypoint, you can first identify the level of this keypoint using the last column of ${\\tt locsDoG}$, and then compute the BRIEF using the image patch from that level of the Gaussian pyramid.\n",
    "\n",
    "The function returns ${\\tt locs}$, an $m \\times 2$ vector, where the two\n",
    "columns are the image coordinates of keypoints, and ${\\tt desc}$, an $m \\times n \\space{ } bits$ matrix of\n",
    "stacked BRIEF descriptors. $m$ is the number of valid descriptors in the image\n",
    "and will vary. $n$ is the number of bits for the BRIEF descriptor. You may have to be careful about the input detector locations\n",
    "since they may be at the edge of an image where we cannot extract a full patch\n",
    "of width $\\texttt{patchWidth}$. Thus, the number of output $\\texttt{locs}$ may be\n",
    "less than the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-arthur",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ba8305e1203454b5f564651dab97e39",
     "grade": false,
     "grade_id": "q2_2_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def computeBrief(gaussPyramid, locsDoG, compareX, compareY):\n",
    "    '''\n",
    "    Compute Brief feature\n",
    "     INPUT\n",
    "     gaussPyramid - the input Gaussian pyramid, (h, w, l).\n",
    "     locsDoG - the keypoints detected by the DOGDetector, (m, 3).\n",
    "     compareX and compareY - linear indices into the \n",
    "                             (patch_width x patch_width) image patch and are\n",
    "                             each (nbits, 1) vectors.\n",
    "                            \n",
    "     OUTPUT\n",
    "     newlocs - an m x 2 vector, where the two columns are the image\n",
    "    \t\t coordinates of keypoints.\n",
    "     desc - an m x n bits matrix of stacked BRIEF descriptors. m is the number\n",
    "            of valid descriptors in the image.\n",
    "    '''\n",
    "    # Hint: you need to:\n",
    "    #  1) Remove those invalid locations (close to the image edges);\n",
    "    #  2) Compute the BRIEF feature for those valid keypoints.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return newlocs, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-friday",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2ac0adb905a5f782c763dbf11ba3aab",
     "grade": true,
     "grade_id": "q2_2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### HIDDEN TEST CELL\n",
    "# Hint: We will use the 'pf_scan_scaled.jpg' for this test.\n",
    "# The output \"newlocs\" and \"desc\" should have the same first dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-constraint",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebd061874075d1eceb8711557cfb9de1",
     "grade": false,
     "grade_id": "q2-note6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3 Putting it all Together (5 pts)\n",
    "\n",
    "#### Q 2.3\n",
    "\n",
    "Write a function :\n",
    "\n",
    "\\begin{equation}\n",
    "{\\tt [locs, desc] = briefLite(im)}\n",
    "\\end{equation}\n",
    "which accepts a grayscale image ${\\tt im}$ with values between zero and one and\n",
    "returns ${\\tt locs}$, an $m \\times 2$ vector, where the two columns are the\n",
    "image coordinates of keypoints, and ${\\tt desc}$, an $m \\times n~bits$ matrix of stacked BRIEF\n",
    "descriptors. $m$ is the number of valid descriptors in the image and will vary.\n",
    "$n$ is the number of bits for the BRIEF descriptor.\n",
    "\n",
    "This function should perform all the necessary steps to extract the descriptors from the image, including\n",
    "   - Load parameters and test patterns\n",
    "   - Get keypoint locations\n",
    "   - Compute a set of valid BRIEF descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-trinity",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14f24e09b6021616aec573c6ce7a6e7e",
     "grade": false,
     "grade_id": "q2_3_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def briefLite(im):\n",
    "    '''\n",
    "    Given an image, detect the keypoints and describe the keypoints with descriptors.\n",
    "    \n",
    "    INPUTS\n",
    "        im - gray image with values between 0 and 1\n",
    "    OUTPUTS\n",
    "        locs - an m x 3 vector, where the first two columns are the image coordinates \n",
    "                of keypoints and the third column is the pyramid level of the keypoints\n",
    "        desc - an m x n bits matrix of stacked BRIEF descriptors. \n",
    "                m is the number of valid descriptors in the image and will vary\n",
    "                n is the number of bits for the BRIEF descriptor\n",
    "    '''\n",
    "    # Hint: Use the provided \"DoGdetector()\" obtain the smoothed Gaussian pyramid and\n",
    "    # the keypoints, and use them as the input of \"computeBrief()\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return locs, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-hampton",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "470af1da813910fa20b60b5a7876db8c",
     "grade": true,
     "grade_id": "q2_3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### HIDDEN TEST CELL\n",
    "# Hint: We will use the 'pf_scan_scaled.jpg' for this test.\n",
    "# The output \"newlocs\" and \"desc\" should have the same first dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-place",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82106d6f5ebe769c4cd26ada04ed5974",
     "grade": false,
     "grade_id": "q2-note7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.4 Check Point: Descriptor Matching (5 pts)\n",
    "\n",
    "A descriptor's strength is in its ability to match to other descriptors\n",
    "generated by the same world point, despite change of view, lighting, etc. The\n",
    "distance metric used to compute the similarity between two descriptors is\n",
    "critical. For BRIEF, this distance metric is the Hamming distance. The Hamming\n",
    "distance is simply the number of bits in two descriptors that differ. (Note that\n",
    "the position of the bits matters.)  \n",
    "\n",
    "\n",
    "To perform the descriptor matching mentioned above, we have provided the function:\n",
    "\n",
    "\\begin{equation}\n",
    "{\\tt [matches] = briefMatch(desc1, desc2, ratio)}\n",
    "\\end{equation}\n",
    "\n",
    "Which accepts an $m_1 \\times n~bits$ stack of BRIEF descriptors from a first image\n",
    "and a $m_2 \\times n~bits$ stack of BRIEF descriptors from a second image and\n",
    "returns a $p \\times 2$ matrix of matches, where the first column are indices\n",
    "into ${\\tt desc1}$ and the second column are indices into ${\\tt desc2}$. Note that\n",
    "$m_1$, $m_2$, and $p$ may be different sizes and $p \\leq \\text{min}\\left(\n",
    "m_1,m_2\n",
    "\\right)$.\n",
    "\n",
    "For a better visual understanding of the result, we have provided a function for\n",
    "you to plot the matched keypoint correspondences:\n",
    "\n",
    "\\begin{equation}\n",
    "{\\tt plotMatches(im1, im2, matches, locs1, locs2)}\n",
    "\\end{equation}\n",
    "\n",
    "where ${\\tt im1}$ and ${\\tt im2}$ are grayscale images from $0$ to $1$,\n",
    "$\\texttt{matches}$ is the list of matches returned by ${\\tt briefMatch}$ and ${\\tt\n",
    "locs1}$ and ${\\tt locs2}$ are the locations of keypoints from ${\\tt briefLite}$.  \n",
    "\n",
    "#### Q 2.4\n",
    "\n",
    "Write a test script ${\\tt testMatch}$ to load\n",
    "two of the $\\textit{chickenbroth}$ images, compute feature matches.\n",
    "\n",
    "**This problem will be manually graded.** Save the resulting figure and submit it in your PDF. Briefly discuss any cases that perform worse or better.\n",
    "\n",
    "Fig 2.1 is an example result.  \n",
    "_Suggest for debugging: A good test of your code is to\n",
    "check that you can match an image to itself._\n",
    "\n",
    "|<img align=\"center\" src=\"figure/matches.png\" width=\"500\">|\n",
    "|:--:|\n",
    "|Fig 2.1 Example of BRIEF matches for model chickenbroth.jpg and chickenbroth\\_01.jpg.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-romania",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d3887a2dbd55560d461868de981a271",
     "grade": false,
     "grade_id": "q2-code5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def briefMatch(desc1, desc2, ratio=0.8):\n",
    "    '''\n",
    "    Performs the descriptor matching.\n",
    "    \n",
    "    INPUTS\n",
    "        desc1 , desc2 - m1 x n and m2 x n matrix. m1 and m2 are the number of keypoints in\n",
    "                        image 1 and 2. n is the number of bits in the brief\n",
    "    OUTPUT\n",
    "        matches - p x 2 matrix. where the first column are indices\n",
    "                  into desc1 and the second column are indices into desc2\n",
    "    '''\n",
    "    D = cdist(np.float32(desc1), np.float32(desc2), metric='hamming')\n",
    "    # find the smallest distance\n",
    "    ix2 = np.argmin(D, axis=1)\n",
    "    d1 = D.min(1)\n",
    "    # find the second smallest distance\n",
    "    d12 = np.partition(D, 2, axis=1)[:,0:2]\n",
    "    d2 = d12.max(1)\n",
    "    r = d1/(d2+1e-10)\n",
    "    is_discr = r<ratio\n",
    "    ix2 = ix2[is_discr]\n",
    "    ix1 = np.arange(D.shape[0])[is_discr]\n",
    "    matches = np.stack((ix1,ix2), axis=-1)\n",
    "    return matches\n",
    "\n",
    "def plotMatches(im1, im2, matches, locs1, locs2):\n",
    "    fig = plt.figure()\n",
    "    # draw two images side by side\n",
    "    imH = max(im1.shape[0], im2.shape[0])\n",
    "    im = np.zeros((imH, im1.shape[1]+im2.shape[1]), dtype='uint8')\n",
    "    im[0:im1.shape[0], 0:im1.shape[1]] = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "    im[0:im2.shape[0], im1.shape[1]:] = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    for i in range(matches.shape[0]):\n",
    "        pt1 = locs1[matches[i,0], 0:2]\n",
    "        pt2 = locs2[matches[i,1], 0:2].copy()\n",
    "        pt2[0] += im1.shape[1]\n",
    "        x = np.asarray([pt1[0], pt2[0]])\n",
    "        y = np.asarray([pt1[1], pt2[1]])\n",
    "        plt.plot(x,y,'r')\n",
    "        plt.plot(x,y,'g.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-ridge",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57b042ca969786fb6778542d92bf5fa0",
     "grade": false,
     "grade_id": "q2-note8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.5 BRIEF and rotations (5 pts)\n",
    "\n",
    "You may have noticed worse performance under rotations. Let's investigate this!\n",
    "\n",
    "#### Q 2.5\n",
    "\n",
    "Take the $\\texttt{model_chickenbroth.jpg}$\n",
    "test image and match it to itself while rotating the second image (hint:\n",
    "$\\texttt{imrotate}$) in increments of 10 degrees. Count the number of correct\n",
    "matches at each rotation and construct a bar graph showing rotation angle vs the\n",
    "number of correct matches.  \n",
    "\n",
    "**This problem will be manually graded.** Include your code and the historgram figure in your PDF, and explain why you think  the descriptor behaves this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-problem",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f321731824ed0df5f77ae1c45b9442e7",
     "grade": false,
     "grade_id": "q2-note9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.6 Improving Performance - (Extra Credit, 10 pts)\n",
    "\n",
    "The extra credit opportunities described below are optional and provide an\n",
    "avenue to explore computer vision and improve the performance of the techniques developed above.\n",
    "\n",
    "**This problem will be manually graded.**\n",
    "\n",
    "   1. ($\\textbf{5 pts}$) As we have seen, BRIEF is not rotation invariant. Design a simple fix to solve this problem using the tools you have developed so far (think back to edge detection and/or Harris corner's covariance matrix).  Include the code in your PDF, and explain your design decisions and how you selected any parameters that you use. Demonstrate the effectiveness of your algorithm on image pairs related by large rotation.\n",
    "\n",
    "   2. ($\\textbf{5 pts}$) This implementation of BRIEF has some scale invariance, but there are limits.  What happens when you match a picture to the same picture at half the size?  Look to section 3 of [Lowe2004](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf) for a technique that will make your detector more robust to changes in scale. Implement it and demonstrate it in action with several test images. Include yout code and the test images in your PDF. You may simply rescale some of the test images we have given you. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
