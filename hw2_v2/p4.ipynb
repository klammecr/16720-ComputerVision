{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96bc3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccc6827",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "#                                    16720 (B) Bag of Visual Words - Assignment 2\n",
    "\n",
    "     Instructor: Kris Kitani                   TAs: Sheng-Yu, Jinkun, Rawal, Arka, Rohan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c30de23a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa34e1bccadddd4af6b000d97fc91852",
     "grade": false,
     "grade_id": "cell-ab714820928ab812",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from skimage import io\n",
    "import skimage.transform\n",
    "import os,time\n",
    "import util\n",
    "import multiprocess\n",
    "import threading\n",
    "import queue\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d50839",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5364b7bd8eec5796ece1fae065f6a05c",
     "grade": false,
     "grade_id": "cell-0943543d897db3c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## For Autograding P4, ensure uploading `trained_conf_matrix.npy` and `trained_system_deep.npz`. \n",
    "\n",
    "## For extra credit, ensure uploading `trained_conf_matrix_vit.npy` and `trained_system_vit.npz`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b983a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3df0d1eeb0e83e225049013b703cb55e",
     "grade": false,
     "grade_id": "cell-17340b02fdb1c6df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Deep Learning Features - An Alternative to ``Bag of Words``\n",
    "\n",
    "As we have discussed in class, another powerful method for scene classification in computer vision is the employment of convolutional neural networks (CNNs) - sometimes referred to generically as $deep learning$. It is important to understand how previously trained (pretrained) networks can be used as another form of feature extraction, and how they relate to classical Bag of Words (BoW) features. We will be covering details on how one chooses the network architecture and training procedures later in the course. For this question, however, we will be asking you to deal with the VGG-16 pretrained network. VGG-16 is a pretrained Convolutional Neural Network (CNN) that has been trained on approximately 1.2 million images from the ImageNet Dataset (``http://image-net.org/index``) by the Visual Geometry Group (VGG) at University of Oxford. The model can classify images into a 1000 object categories (e.g. keyboard, mouse, coffee mug, pencil).\n",
    "\n",
    "One lesson we want you to take away from this exercise is to understand the effectiveness of $deep$ $features$ for general classification tasks within computer vision - even when those features have been previously trained on a different dataset (i.e. ImageNet) and task (i.e. object recognition). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0762f44",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afb19706afe8d0615f44a208e29ff6bd",
     "grade": false,
     "grade_id": "cell-4d7a2d7da9ad6236",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Extracting Deep Features\n",
    "\n",
    "To complete this question, you need to install the ``torchvision`` library from Pytorch, a popular Python-based deep learning library.\n",
    "If you are using the Anaconda package manager (``https://www.anaconda.com/download/``), this can be done with the following command:\n",
    "```\n",
    "            conda install pytorch torchvision -c pytorch\n",
    "```\n",
    "To check that you have installed it correctly, make sure that you can ``import torch`` in a Python interpreter without errors.\n",
    "Please refer to ``https://pytorch.org/`` for more detailed installation instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77d7aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea96722fcf18543140b0ec3d16bd9e2d",
     "grade": false,
     "grade_id": "cell-1086d0481a485a9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q4.1.1 (25 Points)\n",
    "\n",
    "We want to extract out deep features corresponding to the convolutional layers of theVGG-16 network.  In this problem, we will use the trained weights from the VGG network directly, but implement our own operations. To load the network, use the line\n",
    "```\n",
    "        vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "```\n",
    "followed by ``vgg16.eval()``\n",
    "The latter line ensures that the VGG-16 network is in evaluation mode, not training mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1e034",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92298155f306c3a6810a855a830ab225",
     "grade": false,
     "grade_id": "cell-0852d8a76345d947",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "path_img = \"./data/aquarium/sun_aztvjgubyrgvirup.jpg\"\n",
    "\n",
    "image = io.imread(path_img)\n",
    "\n",
    "image = image.astype('float')/255\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "\n",
    "vgg16.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f3be0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28e71afeaf91366424b2b9faf92e5a55",
     "grade": false,
     "grade_id": "cell-8b240fc2fc30262a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We want you to complete a function that is able to output $VGG-16$ network outputs at the **fc7** layer in\n",
    "```\n",
    "    def extract_deep_feature(x,vgg16_weights):\n",
    "```\n",
    "where ``x`` refers to the input image and ``vgg16_weights`` is a structure containing the CNN's network parameters. In this function you will need to write sub-functions ``multichannel_conv2d``, ``relu``, ``max_pool2d``, and ``linear`` corresponding to the fundamental elements of the CNN: multi-channel convolution, rectified linear units (ReLU), max pooling, and fully-connected weighting.\n",
    "\n",
    "We have provided a helper function ``util.get_VGG16_weights()`` that extracts the weight parameters of VGG-16 and its meta information. The returned variable is a numpy array of shape $L\\times 3$, where $L$ is the number of layers in VGG-16. The first column of each row is a string indicating the layer type. The second/third columns may contain the learned weights and biases, or other meta-information (\\eg kernel size of max-pooling). Please refer to the function docstring for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db1a7f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "126bd1545c46831c642e8428f6660a7c",
     "grade": false,
     "grade_id": "cell-5cbd9317d3fa3557",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In order to build the ``extract_deep_feature`` function, you should run a for-loop through each layer index until layer **fc7**, which corresponds to **the second linear layer** (Refer to VGG structure to see where **fc7** is). **Remember**: the output of the preceding layer should be passed through as an input to the next.\n",
    "\n",
    "Details on the sub-functions needed for the ``extract_deep_feature`` function can be found below.\n",
    "\n",
    "Please use ``scipy.ndimage.convolve`` and ``numpy`` functions to implement these functions instead of using pytorch. Please keep speed in mind when implementing your function, for example, using double for loop over pixels is not a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259dc42",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "265efb8ebc1b1970c7cdb98ffd8c36b3",
     "grade": false,
     "grade_id": "cell-536cfed312995b11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "``multichannel_conv2d(x,weight,bias)``:\n",
    "\n",
    "a function that will perform multi-channel 2D convolution which can be defined as follows, \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}^{(j)} = \\sum_{k=1}^{K} \\begin{bmatrix} \\mathbf{x}^{(k)} * \\mathbf{h}^{(j,k)} \\end{bmatrix} + \\mathbf{b}[j] \n",
    "\\end{equation}\n",
    "\n",
    "where $*$ denotes $2D$ convolution, $\\mathbf{x} = \\{ \\mathbf{x}^{(k)} \\}_{k=1}^{K}$ is our vectorized $K$-channel input signal, $\\mathbf{h} = \\{ \\mathbf{h}^{(j,k)} \\}_{k=1,j=1}^{K,J}$ is our $J \\times K$ set of vectorized convolutional filters and $\\mathbf{r} = \\{ \\mathbf{y}^{(j)} \\}_{j=1}^{J}$ is our $J$ channel vectorized output response. Further, unlike traditional single-channel convolution CNNs often append a bias vector $\\mathbf{b}$ whose $J$ elements are added to the $J$ channels of the output response. \n",
    "\n",
    "To implement ``multichannel_conv2d``, you can use the Scipy convolution function directly with for loops to cycle through the filters and channels (``scipy.ndimage.convolve()``). All the necessary details concerning the number of filters ($J$), number of channels ($K$), filter weights ($\\mathbf{h}$) and biases ($\\mathbf{b}$) can be inferred from the shapes/dimensions of the weights and biases. Notice that pytorch's convolution function actually does correlation, so to get similar answer as pytroch with scipy, you need to flip the kernel on both axes using ``np.flip()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "afee3b08",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfafc16377dad8df84358ed123409a7d",
     "grade": false,
     "grade_id": "cell-39e262deb285001e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multichannel_conv2d(x,weight,bias):\n",
    "    '''\n",
    "    Performs multi-channel 2D convolution.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (H,W,input_dim)\n",
    "    * weight: numpy.ndarray of shape (output_dim,input_dim,kernel_size,kernel_size)\n",
    "    * weight is [JxK] where J is the number of filters, K is the number of features\n",
    "    * bias: numpy.ndarray of shape (output_dim)\n",
    "\n",
    "    [output]\n",
    "    * feat: numpy.ndarray of shape (H,W,output_dim)\n",
    "    '''\n",
    "    h, w, input_dims = x.shape\n",
    "    output_dims = weight.shape[0]\n",
    "\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> for 2D convolution we need to use np.fliplr and np.flipud\n",
    "    2.> can use scipy.ndimage.convolve with the flipped kernel\n",
    "    3.> don't forget to add the bias\n",
    "    '''\n",
    "    \n",
    "    channel_filts = []\n",
    "    for img_channel in range(x.shape[2]):\n",
    "        filt_out = []\n",
    "        for filt_num, f in enumerate(weight):\n",
    "            # Convolve the image channel with the filter\n",
    "            filt_out.append(scipy.ndimage.convolve(x[:, :, img_channel], np.fliplr(np.flipud(f[img_channel]))))\n",
    "        channel_filts.append(np.stack(filt_out))\n",
    "    out = np.mean(np.stack(channel_filts), axis=0)\n",
    "    \n",
    "    return np.moveaxis(out, 0, -1) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e5939",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ae1c551b9359d94adf085873f98e4ba",
     "grade": false,
     "grade_id": "cell-244ad83440a13a11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "```relu(x)```:\n",
    "\n",
    "a function that shall perform the Rectified Linear Unit (ReLU) which can be defined mathematically as,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{ReLU}(x) = \\max_{x}(x, 0)\n",
    "\\end{equation}\n",
    "\n",
    "and is applied independently to each element of the matrix/vector $x$ passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "547e2ce4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d0e9f9b71377bf8a52d6526abf7834f",
     "grade": false,
     "grade_id": "cell-9129e8637bed35d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "    Rectified linear unit.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray\n",
    "\n",
    "    [output]\n",
    "    * y: numpy.ndarray\n",
    "    '''\n",
    "    x[x < 0] = 0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec570ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "850072d50afe0d0486db01d3c3dc028e",
     "grade": false,
     "grade_id": "cell-bbe9e052ed717774",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "``max_pool2d(x,size):``\n",
    "\n",
    "a function that shall perform max pooling over $x$ using a receptive field of size $size$ $\\times$ $size$ (we assume a square receptive field here for simplicity).\n",
    "\n",
    "  If the function receives a multi-channel input, then it should apply the max pooling operation across each input channel independently.\n",
    "  \n",
    "(Hint: making use of smart array indexing can drastically speed up the code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "77126b1a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc3d9525fa5233cbcb1a2288e5408d1d",
     "grade": false,
     "grade_id": "cell-c1438c61d632ab1e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "def max_pool2d(x,size):\n",
    "    '''\n",
    "    2D max pooling operation.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (H,W,input_dim)\n",
    "    * size: pooling receptive field\n",
    "\n",
    "    [output]\n",
    "    * y: numpy.ndarray of shape (H/size,W/size,input_dim)\n",
    "    '''\n",
    "    h, w, dims = x.shape\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> estimate the shape you need to apply the pooling operation.\n",
    "    2.> We can smart fill the padding with np.nan and then use np.nanmax to select the max (avoiding nan)\n",
    "    3.> We can input the grid (start_x:end_x, start_y:end_y, dim) as smart array indexing to np.nanmax\n",
    "    '''\n",
    "    # The below code crashes the kernel probably because it takes up too much memory\n",
    "    x_pad = np.zeros((x.shape[0]+size, x.shape[1]+size, x.shape[2]))\n",
    "    for channel in range(x.shape[2]):\n",
    "        x_pad[:, :, channel] = np.pad(x[:, :, channel], int(size/2), mode=\"constant\", constant_values=np.nan)\n",
    "    pool_h, pool_w = h // size, w // size\n",
    "    out_pool = np.zeros((pool_h, pool_w, dims))\n",
    "    \n",
    "    for dim in range(dims):\n",
    "        for i in range(pool_h):\n",
    "            for j in range(pool_w):\n",
    "                out_pool[i, j, dim] = np.nanmax(x_pad[2*i:2*(i+1), 2*j:2*(j+1), dim])\n",
    "    \n",
    "    return out_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f934916",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a30a5e18db579607d17478cb28066cc",
     "grade": false,
     "grade_id": "cell-a0d2f5e224adba0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "``linear(x,W,b):``\n",
    "\n",
    "a function that will compute a node vector where each element is a linear combination of the input nodes, written as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}[j] = \\sum_{k=1}^{K}\\mathbf{W}[j,k] \\mathbf{x}[k] + \\mathbf{b}[j] \n",
    "\\end{equation}\n",
    "\n",
    "or more succinctly in vector form as $\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$ - where $\\mathbf{x}$ is the $(K \\times 1)$ input node vector, $\\mathbf{W}$ is the $(J \\times K)$ weight matrix, $\\mathbf{b}$ is the $(J \\times 1)$ bias vector and $\\mathbf{y}$ is the~$(J \\times 1)$ output node vector.\n",
    "\n",
    "You should not need for-loops to implement this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b10c3744",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83ed8dacb1fb983d4d1f336de7601abc",
     "grade": false,
     "grade_id": "cell-aee1e87151903086",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear(x,W,b):\n",
    "    '''\n",
    "    Fully-connected layer.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (input_dim)\n",
    "    * weight: numpy.ndarray of shape (output_dim,input_dim)\n",
    "    * bias: numpy.ndarray of shape (output_dim)\n",
    "\n",
    "    [output]\n",
    "    * y: numpy.ndarray of shape (output_dim)\n",
    "    '''\n",
    "\n",
    "    y = np.matmul(W, x.flatten()) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c336b9bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "146fa6565ac543a5086ff5c24eeb6585",
     "grade": false,
     "grade_id": "cell-54047ba1b7802e8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should ignore all ``DropoutLayer`` you encounter; they're functional only during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b47ce6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a6f29fbda60148eaaac3fb8e7f4b89d",
     "grade": false,
     "grade_id": "cell-5491f084ad832f2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "VGG-16 assumes that all input imagery to the network is resized to $224 \\times 224$ with the three color channels preserved (use ``skimage.transform.resize()`` to do this before passing any imagery to the network). And be sure to normalize the image using suggested mean and std before extracting the feature:\n",
    "```\n",
    "                                        mean=[0.485,0.456,0.406]}\n",
    "                                        std=[0.229,0.224,0.225]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f96a0ffa",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9656d91025bba1426dd161ada89c41ec",
     "grade": false,
     "grade_id": "cell-7d0ed28aa0f1bffb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from p4_1 import preprocess_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb456f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8725f2cb95e93f82c875bce123ee6c4c",
     "grade": false,
     "grade_id": "cell-1cede50aa78672ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For efficiency you should check that each sub-function is working properly before putting them all together - otherwise it will be hard to track any errors. To compare your implementation with pytroch, you should compare the extracted features between your ``extract_deep_feature``  and the pre-trained VGG-16 network.  ``evaluate_deep_extractor`` should come in handy in comparing the result of the two extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9cc3e81d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c391c1be80d17baab19771e8a946935",
     "grade": false,
     "grade_id": "cell-27d0fcec8607c9f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_deep_feature(x, vgg16_weights):\n",
    "    '''\n",
    "    Extracts deep features from the given VGG-16 weights.\n",
    "\n",
    "    [input]\n",
    "    * x: numpy.ndarray of shape (H,W,3)\n",
    "    * vgg16_weights: list of shape (L,3)\n",
    "\n",
    "    [output]\n",
    "    * feat: numpy.ndarray of shape (K)\n",
    "    '''\n",
    "    \n",
    "    feat = np.copy(x)\n",
    "    for num, layer in enumerate(vgg16_weights):\n",
    "        print(f\"Processing: {layer[0]}\")\n",
    "        if num == 33:\n",
    "            break\n",
    "        elif layer[0] == \"conv2d\":\n",
    "            feat = multichannel_conv2d(feat, layer[1], layer[2])\n",
    "        elif layer[0] == \"relu\":\n",
    "            feat = relu(feat)\n",
    "        elif layer[0] == \"linear\":\n",
    "            feat = linear(feat, layer[1], layer[2])\n",
    "        elif layer[0] == \"maxpool2d\":\n",
    "            feat = max_pool2d(feat, layer[1])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown layer type\")\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "23803b61",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1957f46435c9183a7cb810f8b189c160",
     "grade": false,
     "grade_id": "cell-83df22bcc6adc770",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_deep_extractor(img, vgg16):\n",
    "    '''\n",
    "    Evaluates the deep feature extractor for a single image.\n",
    "\n",
    "    [input]\n",
    "    * image: numpy.ndarray of shape (H,W,3)\n",
    "    * vgg16: prebuilt VGG-16 network.\n",
    "\n",
    "    [output]\n",
    "    * diff: difference between the two feature extractor's result\n",
    "    '''\n",
    "    img_torch = preprocess_image(img)\n",
    "\n",
    "    vgg16_weights = util.get_VGG16_weights()\n",
    "    feat = extract_deep_feature(np.transpose(img_torch.numpy(), (1,2,0)), vgg16_weights)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vgg_classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-3])\n",
    "        vgg_feat_feat = vgg16.features(img_torch[None, ])\n",
    "        vgg_feat_feat = vgg_classifier(vgg_feat_feat.flatten())\n",
    "    \n",
    "    return np.sum(np.abs(vgg_feat_feat.numpy() - feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b68be1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: conv2d\n",
      "Processing: relu\n",
      "Processing: conv2d\n",
      "Processing: relu\n",
      "Processing: maxpool2d\n"
     ]
    }
   ],
   "source": [
    "# NOTE: comment out the lines below before submitting to gradescope\n",
    "# Visible test cases (for debugging)\n",
    "path_img = \"./data/aquarium/sun_aztvjgubyrgvirup.jpg\"\n",
    "image = io.imread(path_img)\n",
    "image = image.astype('float') / 255\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "vgg16.eval()\n",
    "error = evaluate_deep_extractor(image, vgg16)\n",
    "\n",
    "# This error should be less than 1e-10\n",
    "print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5176d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f71d40e5082c99099739ccd3b77548d",
     "grade": false,
     "grade_id": "cell-b841ba750b9c0b46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Building a Visual Recognition System: Revisited\n",
    "\n",
    "We want to compare how useful deep features are in a visual recognition system. Since the speed of the function ``` scipy.ndimage.convolve``` is not ideal, you can use the pytroch VGG-16 network directly (refer to the helper function ```evaluate_deep_extractor``` on how to use the pre-trained network as feature extractor).\n",
    "\n",
    "#### Q4.2.1 (5 Points Autograder + WriteUp):\n",
    "Implement the functions\n",
    "```\n",
    "                    def build_recognition_system(vgg16):\n",
    "```\n",
    "and\n",
    "```\n",
    "                    def eval_recognition_system(vgg16)}\n",
    "```\n",
    "both of which takes the pretrained VGG-16 network as the input arguments. \n",
    "\n",
    "The former function should produce ``trained_system_deep.npz`` as the output. \n",
    "\n",
    "Included will be:\n",
    "* $features$ : a $N \\times  K$ matrix containing all the deep features of the $N$ training images in the data set.\n",
    "* $labels$ : an $N$ vector containing the labels of each of the images. ($features[i]$ will correspond to label $labels[i]$).\n",
    "\n",
    "The latter function should produce the confusion matrix, as with the previous question.\n",
    "\n",
    "Instead of using the histogram intersection similarity, write a function to just use the negative Euclidean distance (as larger values are more similar).\n",
    "\n",
    "**Report the confusion matrix and accuracy for your results in your write-up. Can you comment in your writeup on whether the results are better or worse than classical BoW - why do you think that is?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "29eef544",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8cf20826e56c7b805643fb5e106171b",
     "grade": false,
     "grade_id": "cell-998a996bb091f4c9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from p4_1 import get_image_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7ab97e2d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "690e046304f5925ce2bbfbae43d83828",
     "grade": false,
     "grade_id": "cell-df193a9dbf349af9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from p4_1 import build_recognition_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "357651ab",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca8342e3b0756df670a5dd389cc6fefa",
     "grade": false,
     "grade_id": "cell-c7bb13cc505da9dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from p4_1 import evaluate_recognition_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0cef91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the code\n",
    "# import nbimporter\n",
    "# import torchvision\n",
    "# from p4_1 import preprocess_image\n",
    "\n",
    "# vgg16 = torchvision.models.vgg16(pretrained=True).double()\n",
    "# vgg16.eval()\n",
    "\n",
    "# #ret = build_recognition_system(vgg16, num_workers=4)\n",
    "# conf_matrix, accuracy = evaluate_recognition_system(vgg16)\n",
    "# # We expect the accuracy to be greater than 0.9\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# # Visualize the confusion matrix\n",
    "# def visualize_conf_matrix(conf_matrix):   \n",
    "#     import seaborn as sns\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     labels = [\"AQRM\", \"PARK\", \"DSRT\", \"HGHWY\", \"KTCHN\", \"LNDROMT\", \"WFLL\", \"WNDMLL\"]\n",
    "#     ax = sns.heatmap(conf_matrix, annot=True, cmap='Blues')\n",
    "#     ax.set_title('Seaborn Confusion Matrix with labels\\n\\n')\n",
    "#     ax.set_xlabel('\\nPredicted Values')\n",
    "#     ax.set_ylabel('Actual Values ')\n",
    "#     ax.xaxis.set_ticklabels(labels)\n",
    "#     ax.yaxis.set_ticklabels(labels)\n",
    "#     plt.show()\n",
    "\n",
    "# print(\"My Confusion Matrix:\")\n",
    "# visualize_conf_matrix(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc33169",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87495a23809045f32a4753b8ccb39bec",
     "grade": false,
     "grade_id": "cell-b44920091955aca2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (Extra Credit) Extract ViT features\n",
    "\n",
    "**q4.3.1 (10 points)**\n",
    "For extra credit, we ask you to try out the features from a recently popular architecture -- Vision Transformers (ViTs).\n",
    "\n",
    "You'll learn more about ViT later in the course. As an overview, instead of convolutions, ViT treat image patches as tokens, and process the tokens via self-attention. For more details, feel free to check out the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (``https://arxiv.org/abs/2010.11929``).\n",
    "\n",
    "The goal in this problem is not to understand ViT. We expect you to learn how to extract features from an unseen architecture (e.g., a new model released straight out of arxiv). Here you'll learn how to:\n",
    "1. Iterate through the pytorch nn modules without digging too much into the source code.\n",
    "2. Learn another way to extract features -- `register_forward_hook()`\n",
    "\n",
    "In this problem, we'll be using an ImageNet-pretrained ViT from `pytorch_pretrained_vit`. In case you haven't installed it, run:\n",
    "\n",
    "```pip install pytorch_pretrained_vit```\n",
    "\n",
    "Below is a script that loads a pre-trained ViT and prints out all the submodules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b8d0178c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7062c6d191ebd6e507517c5eeb95c3b",
     "grade": false,
     "grade_id": "cell-0427f5646c9df273",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chris\\dev\\16720-ComputerVision\\hw2_v2\\p4.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_pretrained_vit\u001b[39;00m \u001b[39mimport\u001b[39;00m ViT\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# loads a pretrained ViT\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m vit_model \u001b[39m=\u001b[39m ViT(\u001b[39m'\u001b[39;49m\u001b[39mB_16_imagenet1k\u001b[39;49m\u001b[39m'\u001b[39;49m, pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m vit_model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# iterate through the submodules and print out names\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\pytorch_pretrained_vit\\model.py:124\u001b[0m, in \u001b[0;36mViT.__init__\u001b[1;34m(self, name, pretrained, patches, dim, ff_dim, num_heads, num_layers, attention_dropout_rate, dropout_rate, representation_size, load_repr_layer, classifier, positional_embedding, in_channels, image_size, num_classes)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(pre_logits_size, num_classes)\n\u001b[0;32m    123\u001b[0m \u001b[39m# Initialize weights\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_weights()\n\u001b[0;32m    126\u001b[0m \u001b[39m# Load pretrained model\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[39mif\u001b[39;00m pretrained:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[1;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\pytorch_pretrained_vit\\model.py:146\u001b[0m, in \u001b[0;36mViT.init_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(m, \u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m             nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mnormal_(m\u001b[39m.\u001b[39mbias, std\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)  \u001b[39m# nn.init.constant(m.bias, 0)\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(_init)\n\u001b[0;32m    147\u001b[0m nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mconstant_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc\u001b[39m.\u001b[39mweight, \u001b[39m0\u001b[39m)\n\u001b[0;32m    148\u001b[0m nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mconstant_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc\u001b[39m.\u001b[39mbias, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:659\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[39m    )\u001b[39;00m\n\u001b[0;32m    657\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 659\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[0;32m    660\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[0;32m    661\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:659\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[39m    )\u001b[39;00m\n\u001b[0;32m    657\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 659\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[0;32m    660\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[0;32m    661\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "    \u001b[1;31m[... skipping similar frames: Module.apply at line 659 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:659\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[39m    )\u001b[39;00m\n\u001b[0;32m    657\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 659\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[0;32m    660\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[0;32m    661\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:660\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m    659\u001b[0m     module\u001b[39m.\u001b[39mapply(fn)\n\u001b[1;32m--> 660\u001b[0m fn(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    661\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\pytorch_pretrained_vit\\model.py:143\u001b[0m, in \u001b[0;36mViT.init_weights.<locals>._init\u001b[1;34m(m)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init\u001b[39m(m):\n\u001b[0;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(m, nn\u001b[39m.\u001b[39mLinear):\n\u001b[1;32m--> 143\u001b[0m         nn\u001b[39m.\u001b[39;49minit\u001b[39m.\u001b[39;49mxavier_uniform_(m\u001b[39m.\u001b[39;49mweight)  \u001b[39m# _trunc_normal(m.weight, std=0.02)  # from .initialization import _trunc_normal\u001b[39;00m\n\u001b[0;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(m, \u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m             nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mnormal_(m\u001b[39m.\u001b[39mbias, std\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\torch\\nn\\init.py:321\u001b[0m, in \u001b[0;36mxavier_uniform_\u001b[1;34m(tensor, gain)\u001b[0m\n\u001b[0;32m    318\u001b[0m std \u001b[39m=\u001b[39m gain \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m2.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(fan_in \u001b[39m+\u001b[39m fan_out))\n\u001b[0;32m    319\u001b[0m a \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m \u001b[39mreturn\u001b[39;00m _no_grad_uniform_(tensor, \u001b[39m-\u001b[39;49ma, a)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\torch\\nn\\init.py:14\u001b[0m, in \u001b[0;36m_no_grad_uniform_\u001b[1;34m(tensor, a, b)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_uniform_\u001b[39m(tensor, a, b):\n\u001b[0;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 14\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(a, b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_vit import ViT\n",
    "\n",
    "\n",
    "# loads a pretrained ViT\n",
    "vit_model = ViT('B_16_imagenet1k', pretrained=True)\n",
    "vit_model.eval()\n",
    "\n",
    "# iterate through the submodules and print out names\n",
    "for name, module in vit_model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb9d41",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "794ef7d63fe149d6ad49102a6f06abfa",
     "grade": false,
     "grade_id": "cell-f4161890fb1ae55f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`named_modules()` will recursively find out all the `nn.Modules` defined inside `vit_model`. For example: \n",
    "1. `vit_model` has children `patch_embedding`, `positional_embedding`, `transformer`, `norm`, `fc`.\n",
    "2. Inside the `transformer` module, `transformer` has a child `transformer.blocks`.\n",
    "3. `transformer.blocks` module has children `transformer.blocks.0`, `transformer.blocks.1`, etc.\n",
    "4. The logic applies recursively.\n",
    "\n",
    "Unfortunately, calling just `named_modules()` will not give you the information about the ordering each module is used.\n",
    "\n",
    "However, if you have a basic understanding of the architecture code, you can extract features quickly, regardless of how complicated the `forward()` function is. In ViT, we know that the `patch_embedding` and `positional_embedding` is passed into `transformer`, and the output of `transformer` is then passed into `norm` then `fc`.\n",
    "\n",
    "Here, we want to extract features from the output of the `transformer` module. We will use `register_forward_hook()` to do this. Please read the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=forward%20hook#torch.nn.Module.register_forward_hook) for details. Essentially, we want to \"hook\" a function that records the output of the `transformer` module, whenever `vit_model` is called. We'll define a class FeatureExtractor to do this.\n",
    "\n",
    "**Note:** The input image size for ViT will be 384x384, and normalization uses a different mean and std:\n",
    "```\n",
    "                                        mean=[0.5, 0.5, 0.5]\n",
    "                                        std=[0.5, 0.5, 0.5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea28d3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b0206d7b4eafe8f23aefa28ffe7c0a5",
     "grade": false,
     "grade_id": "cell-a6e05e47e0061913",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from p4_1 import preprocess_image\n",
    "\n",
    "class FeatureExtractor:\n",
    "    '''\n",
    "    A class that takes in a nn.Module model and extracts feature from specified layer name.\n",
    "    '''\n",
    "    def __init__(self, model, layername='transformer'):\n",
    "        self.extracted_feature = None\n",
    "        self.model = model              # This will be vit_model in our case\n",
    "        self.layername = layername\n",
    "\n",
    "        # Apply hook to the transformer module\n",
    "        '''\n",
    "        HINTS:\n",
    "        1.> The for loop of named_modules() we provided will be useful here.\n",
    "        2.> Apply feature_extract_hook() to the transformer module using register_forward_hook()\n",
    "        '''\n",
    "        for name, module in model.named_modules():\n",
    "            # Register the forward hook\n",
    "            if name == layername:\n",
    "                module.blocks[-1].norm2.register_forward_hook(self.feature_extract_hook)\n",
    "\n",
    "    def feature_extract_hook(self, module, input, output):\n",
    "        '''\n",
    "        A function hook that extracts the module's output to the global variable `extracted_feature`\n",
    "\n",
    "        [input]\n",
    "        * module: module of interest\n",
    "        * input: input of the module\n",
    "        * output: output of the module\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        HINTS:\n",
    "        1.> You don't need to use all the arguments in this function.\n",
    "        2.> What you need to do in this function should be really simple.\n",
    "        '''\n",
    "        self.extracted_feature = output.detach()\n",
    "\n",
    "    def extract_feature(self, img):\n",
    "        '''\n",
    "        Takes in an image, feed it to the model, and outputs the desired feature.\n",
    "        \n",
    "        [input]\n",
    "        * x: preprocessed image\n",
    "        \n",
    "        [output]\n",
    "        * feature: feature extracted from the specified layer name\n",
    "        '''\n",
    "        x = preprocess_image_vit(img).float()\n",
    "        \n",
    "        # simply run a forward pass of the model\n",
    "        with torch.no_grad():\n",
    "            self.model.forward(x.unsqueeze(0))\n",
    "\n",
    "        # feature will be extracted in self.extracted_feature already, thanks to the hook\n",
    "        # you might wonder why we take only part of the output as feature\n",
    "        # this is because we are only using the \"class token\" as the feature\n",
    "        # for more details, please read the paper!\n",
    "        return self.extracted_feature.numpy()[0, 0]\n",
    "\n",
    "\n",
    "def preprocess_image_vit(image):\n",
    "    '''\n",
    "    Preprocesses the image to load into the prebuilt network.\n",
    "\n",
    "    [input]\n",
    "    * image: numpy.ndarray of shape (H,W,3)\n",
    "\n",
    "    [output]\n",
    "    * image_processed: torch.array of shape (3,H,W)\n",
    "    '''\n",
    "    return preprocess_image(image, size = (384, 384, 3), mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ae58e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcc66c67600645f39e024fe8e885cd3c",
     "grade": false,
     "grade_id": "cell-e7ac1eb05fbec3c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**q4.3.2 (5 points + Write up)** Now we can extract features from ViT to build our recognition system. The following code will be mostly the same as the one you did before.\n",
    "\n",
    "Write up: Report the confusion matrix and accuracy for your results in your write-up. Can you comment in your writeup on whether the results are better or worse than VGG - why do you think that is? A short answer is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c0f23",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efbde40fdfb4c984a97453164013ba76",
     "grade": false,
     "grade_id": "cell-2c426f5044472a37",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def build_recognition_system_vit(vit_feat_extractor):\n",
    "    '''\n",
    "    Creates a trained recognition system by generating training features from all training images.\n",
    "\n",
    "    [input]\n",
    "    * vit_feat_extractor: feature extractor for ViT\n",
    "\n",
    "    [saved]\n",
    "    * features: numpy.ndarray of shape (N,K)\n",
    "    * labels: numpy.ndarray of shape (N)\n",
    "    '''\n",
    "\n",
    "    train_data = np.load(\"./data/train_data.npz\", allow_pickle=True)\n",
    "    args = []\n",
    "\n",
    "    # Parse out the training files\n",
    "    train_files = train_data.get(\"files\")\n",
    "    if train_files is None:\n",
    "        raise ValueError(\"No valid training files available :(\")\n",
    "    train_files = [\"./data/\" + str(file) for file in train_files]\n",
    "\n",
    "    # Do the processing\n",
    "    out  = np.zeros((len(train_files), 768))\n",
    "    for idx, train_file in enumerate(train_files):\n",
    "        out[idx] = vit_feat_extractor.extract_feature(skimage.io.imread(train_file))\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> reorder the features to their correct place as input\n",
    "    '''\n",
    "    print(\"done\", out.shape)\n",
    "    labels = train_data.get(\"labels\")\n",
    "    np.savez('trained_system_vit.npz', features=out, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9234f15",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7199e4b9d4d6508e069d662914a5ef8d",
     "grade": false,
     "grade_id": "cell-76c948b5431eabdb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from p3 import distance_to_set\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_recognition_system_vit(vit_feat_extractor):\n",
    "    '''\n",
    "    Evaluates the recognition system for all test images and returns the confusion matrix.\n",
    "\n",
    "    [input]\n",
    "    * vit_feat_extractor: feature extractor for ViT\n",
    "\n",
    "    [output]\n",
    "    * conf: numpy.ndarray of shape (8,8)\n",
    "    * accuracy: accuracy of the evaluated system\n",
    "    '''\n",
    "\n",
    "    test_data = np.load(\"./data/test_data.npz\")\n",
    "    trained_system = np.load(\"trained_system_vit.npz\")\n",
    "    \n",
    "    image_names = test_data['files']\n",
    "    image_names = [\"./data/\" + str(file) for file in image_names]\n",
    "    test_labels = test_data['labels']\n",
    "\n",
    "    trained_features = trained_system['features']\n",
    "    train_labels = trained_system['labels']\n",
    "    print(\"Trained features shape: \", trained_features.shape)\n",
    "    \n",
    "    # Gather arguments for multiprocessing\n",
    "    args = []\n",
    "    for idx, img in enumerate(image_names):\n",
    "        args.append((idx, img))\n",
    "\n",
    "    # Dictionary to hold on to our predictions\n",
    "    pred_labels = []\n",
    "\n",
    "    for arg in args:\n",
    "        # Extract the feature for the test image\n",
    "        test_features = vit_feat_extractor.extract_feature(skimage.io.imread(arg[1]))\n",
    "\n",
    "        # Calculate the distance between the two features\n",
    "        dist = np.sum((test_features - trained_features) ** 2, axis = 1)\n",
    "        pred_label = train_labels[np.argmin(dist)]\n",
    "        pred_labels.append(pred_label)\n",
    "\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    print(\"Predicted labels shape: \", pred_labels.shape)\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    num_labels  = len(np.unique(test_labels))\n",
    "    conf_matrix = np.zeros((num_labels, num_labels))\n",
    "\n",
    "    for i in range(len(test_labels)):\n",
    "        conf_matrix[test_labels[i], pred_labels[i]] += 1\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = np.trace(conf_matrix) / len(image_names)\n",
    "\n",
    "    # Visualize the confusion matrix\n",
    "    def visualize_conf_matrix(conf_matrix):   \n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        labels = [\"AQRM\", \"PARK\", \"DSRT\", \"HGHWY\", \"KTCHN\", \"LNDROMT\", \"WFLL\", \"WNDMLL\"]\n",
    "        ax = sns.heatmap(conf_matrix, annot=True, cmap='Blues')\n",
    "        ax.set_title('Seaborn Confusion Matrix with labels\\n\\n')\n",
    "        ax.set_xlabel('\\nPredicted Values')\n",
    "        ax.set_ylabel('Actual Values ')\n",
    "        ax.xaxis.set_ticklabels(labels)\n",
    "        ax.yaxis.set_ticklabels(labels)\n",
    "        plt.show()\n",
    "\n",
    "    print(\"My Confusion Matrix:\")\n",
    "    visualize_conf_matrix(conf_matrix)\n",
    "\n",
    "    # Save confusion matrix to a file\n",
    "    np.save(\"./conf_matrix_vit.npy\", conf_matrix)\n",
    "    return conf_matrix, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e77a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights.\n",
      "Trained features shape:  (1000, 768)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chris\\dev\\16720-ComputerVision\\hw2_v2\\p4.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m vit_feat_extractor \u001b[39m=\u001b[39m FeatureExtractor(vit)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# build_recognition_system_vit(vit_feat_extractor)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m conf_matrix, accuracy \u001b[39m=\u001b[39m evaluate_recognition_system_vit(vit_feat_extractor)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m\"\u001b[39m, accuracy)\n",
      "\u001b[1;32mc:\\Users\\chris\\dev\\16720-ComputerVision\\hw2_v2\\p4.ipynb Cell 38\u001b[0m in \u001b[0;36mevaluate_recognition_system_vit\u001b[1;34m(vit_feat_extractor)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m pred_labels \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m# Extract the feature for the test image\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     test_features \u001b[39m=\u001b[39m vit_feat_extractor\u001b[39m.\u001b[39;49mextract_feature(skimage\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mimread(arg[\u001b[39m1\u001b[39;49m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# Calculate the distance between the two features\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     dist \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum((test_features \u001b[39m-\u001b[39m trained_features) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\chris\\dev\\16720-ComputerVision\\hw2_v2\\p4.ipynb Cell 38\u001b[0m in \u001b[0;36mFeatureExtractor.extract_feature\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_feature\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m    Takes in an image, feed it to the model, and outputs the desired feature.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m    * feature: feature extracted from the specified layer name\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     x \u001b[39m=\u001b[39m preprocess_image_vit(img)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39m# simply run a forward pass of the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32mc:\\Users\\chris\\dev\\16720-ComputerVision\\hw2_v2\\p4.ipynb Cell 38\u001b[0m in \u001b[0;36mpreprocess_image_vit\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_image_vit\u001b[39m(image):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m    Preprocesses the image to load into the prebuilt network.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m    * image_processed: torch.array of shape (3,H,W)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/dev/16720-ComputerVision/hw2_v2/p4.ipynb#X52sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m preprocess_image(image, size \u001b[39m=\u001b[39;49m (\u001b[39m384\u001b[39;49m, \u001b[39m384\u001b[39;49m, \u001b[39m3\u001b[39;49m), mean \u001b[39m=\u001b[39;49m [\u001b[39m0.5\u001b[39;49m, \u001b[39m0.5\u001b[39;49m, \u001b[39m0.5\u001b[39;49m], std \u001b[39m=\u001b[39;49m [\u001b[39m0.5\u001b[39;49m, \u001b[39m0.5\u001b[39;49m, \u001b[39m0.5\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\chris\\dev\\16720-ComputerVision\\hw2_v2\\p4_1.py:42\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[1;34m(image, size, mean, std)\u001b[0m\n\u001b[0;32m     35\u001b[0m     image \u001b[39m=\u001b[39m image[:, :, \u001b[39m0\u001b[39m:\u001b[39m3\u001b[39m]\n\u001b[0;32m     36\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mHINTS:\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m1.> Resize the image (look into skimage.transform.resize)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39m2.> normalize the image\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m3.> convert the image from numpy to torch\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m resized_img \u001b[39m=\u001b[39m skimage\u001b[39m.\u001b[39;49mtransform\u001b[39m.\u001b[39;49mresize(image, size)\n\u001b[0;32m     43\u001b[0m resized_img \u001b[39m=\u001b[39m resized_img\u001b[39m.\u001b[39mtranspose((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     45\u001b[0m \u001b[39m# Create mean \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\skimage\\transform\\_warps.py:187\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m NumpyVersion(scipy\u001b[39m.\u001b[39m__version__) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1.6.0\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    185\u001b[0m     \u001b[39m# The grid_mode kwarg was introduced in SciPy 1.6.0\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     zoom_factors \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m factors]\n\u001b[1;32m--> 187\u001b[0m     out \u001b[39m=\u001b[39m ndi\u001b[39m.\u001b[39;49mzoom(image, zoom_factors, order\u001b[39m=\u001b[39;49morder, mode\u001b[39m=\u001b[39;49mndi_mode,\n\u001b[0;32m    188\u001b[0m                    cval\u001b[39m=\u001b[39;49mcval, grid_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    190\u001b[0m \u001b[39m# TODO: Remove the fallback code below once SciPy >= 1.6.0 is required.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \n\u001b[0;32m    192\u001b[0m \u001b[39m# 2-dimensional interpolation\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(output_shape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m (\u001b[39mlen\u001b[39m(output_shape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    194\u001b[0m                                 output_shape[\u001b[39m2\u001b[39m] \u001b[39m==\u001b[39m input_shape[\u001b[39m2\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\vision_conda\\lib\\site-packages\\scipy\\ndimage\\_interpolation.py:816\u001b[0m, in \u001b[0;36mzoom\u001b[1;34m(input, zoom, output, order, mode, cval, prefilter, grid_mode)\u001b[0m\n\u001b[0;32m    812\u001b[0m zoom \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mdivide(zoom_nominator, zoom_div,\n\u001b[0;32m    813\u001b[0m                     out\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mones_like(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mfloat64),\n\u001b[0;32m    814\u001b[0m                     where\u001b[39m=\u001b[39mzoom_div \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m    815\u001b[0m zoom \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mascontiguousarray(zoom)\n\u001b[1;32m--> 816\u001b[0m _nd_image\u001b[39m.\u001b[39;49mzoom_shift(filtered, zoom, \u001b[39mNone\u001b[39;49;00m, output, order, mode, cval, npad,\n\u001b[0;32m    817\u001b[0m                      grid_mode)\n\u001b[0;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NOTE: comment out the lines below before submitting to gradescope\n",
    "### Run the code\n",
    "vit = ViT('B_16_imagenet1k', pretrained=True)\n",
    "vit.eval()\n",
    "vit_feat_extractor = FeatureExtractor(vit)\n",
    "# build_recognition_system_vit(vit_feat_extractor)\n",
    "conf_matrix, accuracy = evaluate_recognition_system_vit(vit_feat_extractor)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c067d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "139b99f8d6775c060c31a5fd340b9143",
     "grade": true,
     "grade_id": "q_4_1_1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da44bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c7cf16f5994d47f23c4c127f7a318a5",
     "grade": true,
     "grade_id": "q_4_1_2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7cd3b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce812af4b046483ab1e43c6792b4448d",
     "grade": true,
     "grade_id": "q_4_1_3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232d06c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19d4948aa8f9c3ce2b4181d9919d68b1",
     "grade": true,
     "grade_id": "q_4_1_4",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4d510",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02a0e96d948d22209c3b3a5626ae40b6",
     "grade": true,
     "grade_id": "q_4_1_5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbfc3c9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7555970fe39f565b777d4c8f37478396",
     "grade": true,
     "grade_id": "q_4_1_6",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afecd654",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40e815d75f7923b2bc81403f5cf53bc8",
     "grade": true,
     "grade_id": "q_4_2_2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51bfcc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae0796b60f35e32d4449672587cdc6ea",
     "grade": true,
     "grade_id": "q_4_3_1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e257837",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8772a12ca5fc97a77ef86939fc574174",
     "grade": true,
     "grade_id": "q_4_3_2",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6ad70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "877ef725bb5eae4c6cf7678e093d7ffd",
     "grade": true,
     "grade_id": "q_4_3_3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19103772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('vision_conda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab1be24f2b69fea20ab96b72f6a75a8226e3980324f891cd62f88ac8e8b7a219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
